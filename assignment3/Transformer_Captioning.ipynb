{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-XHTF-_Z1tS2"
   },
   "outputs": [],
   "source": [
    "# # This mounts your Google Drive to the Colab VM.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# # assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
    "# FOLDERNAME = 'cs231n/assignments/assignment3/'\n",
    "# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# # Now that we've mounted your Drive, this ensures that\n",
    "# # the Python interpreter of the Colab VM can load\n",
    "# # python files from within it.\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# # This downloads the COCO dataset to your Drive\n",
    "# # if it doesn't already exist.\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "# # !bash get_datasets.sh\n",
    "# !bash get_coco_dataset.sh\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRvYckCI7qyX",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Image Captioning with Transformers\n",
    "You have now implemented a vanilla RNN and for the task of image captioning. In this notebook you will implement key pieces of a transformer decoder to accomplish the same task.\n",
    "\n",
    "**NOTE:** This notebook will be primarily written in PyTorch rather than NumPy, unlike the RNN notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FXCykho37qya",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup cell.\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.transformer_layers import *\n",
    "from cs231n.captioning_solver_transformer import CaptioningSolverTransformer\n",
    "from cs231n.classifiers.transformer import CaptioningTransformer\n",
    "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from cs231n.image_utils import image_from_url\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # Set default size of plots.\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fczxReWM7qyd"
   },
   "source": [
    "# COCO Dataset\n",
    "As in the previous notebooks, we will use the COCO dataset for captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lCKchFVJ7qye"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base dir  D:\\study\\math-cs-ai\\cs231n\\assignment3\\cs231n\\datasets/coco_captioning\n",
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "idx_to_word <class 'list'> 1004\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n"
     ]
    }
   ],
   "source": [
    "# Load COCO data from disk into a dictionary.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "# Print out all the keys and values from the data dictionary.\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go1XDgIn7qyg"
   },
   "source": [
    "# Transformer\n",
    "As you have seen, RNNs are incredibly powerful but often slow to train. Further, RNNs struggle to encode long-range dependencies (though LSTMs are one way of mitigating the issue). In 2017, Vaswani et al introduced the Transformer in their paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) to a) introduce parallelism and b) allow models to learn long-range dependencies. The paper not only led to famous models like BERT and GPT in the natural language processing community, but also an explosion of interest across fields, including vision. While here we introduce the model in the context of image captioning, the idea of attention itself is much more general.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqPMDm4F9m0v"
   },
   "source": [
    "# Transformer: Multi-Headed Attention\n",
    "\n",
    "### Dot-Product Attention\n",
    "\n",
    "Recall that attention can be viewed as an operation on a query $q\\in\\mathbb{R}^d$, a set of value vectors $\\{v_1,\\dots,v_n\\}, v_i\\in\\mathbb{R}^d$, and a set of key vectors $\\{k_1,\\dots,k_n\\}, k_i \\in \\mathbb{R}^d$, specified as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91g6XLTr7qyi"
   },
   "source": [
    "\\begin{align}\n",
    "c &= \\sum_{i=1}^{n} v_i \\alpha_i \\\\\n",
    "\\alpha_i &= \\frac{\\exp(k_i^\\top q)}{\\sum_{j=1}^{n} \\exp(k_j^\\top q)} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D2wcdeS7qyj"
   },
   "source": [
    "where $\\alpha_i$ are frequently called the \"attention weights\", and the output $c\\in\\mathbb{R}^d$ is a correspondingly weighted average over the value vectors.\n",
    "\n",
    "### Self-Attention\n",
    "In Transformers, we perform self-attention, which means that the values, keys and query are derived from the input $X \\in \\mathbb{R}^{\\ell \\times d}$, where $\\ell$ is our sequence length. Specifically, we learn parameter matrices $V,K,Q \\in \\mathbb{R}^{d\\times d}$ to map our input $X$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2JDsjFU7qyl"
   },
   "source": [
    "\\begin{align}\n",
    "v_i = Vx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\\\\\n",
    "k_i = Kx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\\\\\n",
    "q_i = Qx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5GNjXoW7qyn"
   },
   "source": [
    "### Multi-Headed Scaled Dot-Product Attention\n",
    "In the case of multi-headed attention, we learn a parameter matrix for each head, which gives the model more expressivity to attend to different parts of the input. Let $h$ be number of heads, and $Y_i$ be the attention output of head $i$. Thus we learn individual matrices $Q_i$, $K_i$ and $V_i$. To keep our overall computation the same as the single-headed case, we choose $Q_i \\in \\mathbb{R}^{d\\times d/h}$, $K_i \\in \\mathbb{R}^{d\\times d/h}$ and $V_i \\in \\mathbb{R}^{d\\times d/h}$. Adding in a scaling term $\\frac{1}{\\sqrt{d/h}}$ to our simple dot-product attention above, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRyHp98f7qyo"
   },
   "source": [
    "\\begin{equation}\n",
    "Y_i = \\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)(XV_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzKHMc3g7qyo"
   },
   "source": [
    "where $Y_i\\in\\mathbb{R}^{\\ell \\times d/h}$, where $\\ell$ is our sequence length.\n",
    "\n",
    "In our implementation, we apply dropout to the attention weights (though in practice it could be used at any step):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJFFO4uu7qyp"
   },
   "source": [
    "\\begin{equation}\n",
    "Y_i = \\text{dropout}\\bigg(\\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)\\bigg)(XV_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KlyhrBW7qyp"
   },
   "source": [
    "Finally, then the output of the self-attention is a linear transformation of the concatenation of the heads:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFR0osPf7qyq"
   },
   "source": [
    "\\begin{equation}\n",
    "Y = [Y_1;\\dots;Y_h]A\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4G9fKfW7qyq"
   },
   "source": [
    "were $A \\in\\mathbb{R}^{d\\times d}$ and $[Y_1;\\dots;Y_h]\\in\\mathbb{R}^{\\ell \\times d}$.\n",
    "\n",
    "Implement multi-headed scaled dot-product attention in the `MultiHeadAttention` class in the file `cs231n/transformer_layers.py`. The code below will check your implementation. The relative error should be less than `e-3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VeixCEKF7qyr",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_attn_output error:  0.4425428834738571\n",
      "masked_self_attn_output error:  0.5352395576063157\n",
      "attn_output error:  1.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(231)\n",
    "\n",
    "# Choose dimensions such that they are all unique for easier debugging:\n",
    "# Specifically, the following values correspond to N=1, H=2, T=3, E//H=4, and E=8.\n",
    "batch_size = 1\n",
    "sequence_length = 3\n",
    "embed_dim = 8\n",
    "attn = MultiHeadAttention(embed_dim, num_heads=2)\n",
    "\n",
    "# Self-attention.\n",
    "data = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "self_attn_output = attn(query=data, key=data, value=data)\n",
    "\n",
    "# Masked self-attention.\n",
    "mask = torch.randn(sequence_length, sequence_length) < 0.5\n",
    "masked_self_attn_output = attn(query=data, key=data, value=data, attn_mask=mask)\n",
    "\n",
    "# Attention using two inputs.\n",
    "other_data = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "attn_output = attn(query=data, key=other_data, value=other_data)\n",
    "\n",
    "expected_self_attn_output = np.asarray([[\n",
    "[-0.2494,  0.1396,  0.4323, -0.2411, -0.1547,  0.2329, -0.1936,\n",
    "          -0.1444],\n",
    "         [-0.1997,  0.1746,  0.7377, -0.3549, -0.2657,  0.2693, -0.2541,\n",
    "          -0.2476],\n",
    "         [-0.0625,  0.1503,  0.7572, -0.3974, -0.1681,  0.2168, -0.2478,\n",
    "          -0.3038]]])\n",
    "\n",
    "expected_masked_self_attn_output = np.asarray([[\n",
    "[-0.1347,  0.1934,  0.8628, -0.4903, -0.2614,  0.2798, -0.2586,\n",
    "          -0.3019],\n",
    "         [-0.1013,  0.3111,  0.5783, -0.3248, -0.3842,  0.1482, -0.3628,\n",
    "          -0.1496],\n",
    "         [-0.2071,  0.1669,  0.7097, -0.3152, -0.3136,  0.2520, -0.2774,\n",
    "          -0.2208]]])\n",
    "\n",
    "expected_attn_output = np.asarray([[\n",
    "[-0.1980,  0.4083,  0.1968, -0.3477,  0.0321,  0.4258, -0.8972,\n",
    "          -0.2744],\n",
    "         [-0.1603,  0.4155,  0.2295, -0.3485, -0.0341,  0.3929, -0.8248,\n",
    "          -0.2767],\n",
    "         [-0.0908,  0.4113,  0.3017, -0.3539, -0.1020,  0.3784, -0.7189,\n",
    "          -0.2912]]])\n",
    "\n",
    "print('self_attn_output error: ', rel_error(expected_self_attn_output, self_attn_output.detach().numpy()))\n",
    "print('masked_self_attn_output error: ', rel_error(expected_masked_self_attn_output, masked_self_attn_output.detach().numpy()))\n",
    "print('attn_output error: ', rel_error(expected_attn_output, attn_output.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcDBRnqL9m0w"
   },
   "source": [
    "# Positional Encoding\n",
    "\n",
    "While transformers are able to easily attend to any part of their input, the attention mechanism has no concept of token order. However, for many tasks (especially natural language processing), relative token order is very important. To recover this, the authors add a positional encoding to the embeddings of individual word tokens.\n",
    "\n",
    "Let us define a matrix $P \\in \\mathbb{R}^{l\\times d}$, where $P_{ij} = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zawv8vzV7qyt"
   },
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\text{sin}\\left(i \\cdot 10000^{-\\frac{j}{d}}\\right) & \\text{if j is even} \\\\\n",
    "\\text{cos}\\left(i \\cdot 10000^{-\\frac{(j-1)}{d}}\\right) & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0fhlupT7qyt"
   },
   "source": [
    "Rather than directly passing an input $X \\in \\mathbb{R}^{l\\times d}$ to our network, we instead pass $X + P$.\n",
    "\n",
    "Implement this layer in `PositionalEncoding` in `cs231n/transformer_layers.py`. Once you are done, run the following to perform a simple test of your implementation. You should see errors on the order of `e-3` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gi7px0XK7qyu",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe_output error:  1.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(231)\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 2\n",
    "embed_dim = 6\n",
    "data = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "\n",
    "pos_encoder = PositionalEncoding(embed_dim)\n",
    "output = pos_encoder(data)\n",
    "\n",
    "expected_pe_output = np.asarray([[[-1.2340,  1.1127,  1.6978, -0.0865, -0.0000,  1.2728],\n",
    "                                  [ 0.9028, -0.4781,  0.5535,  0.8133,  1.2644,  1.7034]]])\n",
    "\n",
    "print('pe_output error: ', rel_error(expected_pe_output, output.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDoSUJ7y7qyv",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "# Inline Question 1\n",
    "\n",
    "Several key design decisions were made in designing the scaled dot product attention we introduced above. Explain why the following choices were beneficial:\n",
    "1. Using multiple attention heads as opposed to one.\n",
    "2. Dividing by $\\sqrt{d/h}$ before applying the softmax function. Recall that $d$ is the feature dimension and $h$ is the number of heads.\n",
    "3. Adding a linear transformation to the output of the attention operation.\n",
    "\n",
    "Only one or two sentences per choice is necessary, but be sure to be specific in addressing what would have happened without each given implementation detail, why such a situation would be suboptimal, and how the proposed implementation improves the situation.\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "\n",
    "1. **使用多个注意力头能让模型在不同的“表示子空间”（representation subspaces）中并行地学习多种类型的相关性**。相比之下，单个注意力头只能学习一种混合的、平均化的关系，可能会忽略掉特定角度的丰富信息。例如，一个头可能专注于学习语法结构，而另一个头则学习语义相似性，多头机制允许模型同时捕捉这些不同维度的信息，从而获得更全面的理解。\n",
    "\n",
    "2. **除以 $\\sqrt{d/h}$（即每个头的维度 $d_k$ 的平方根）是为了对注意力得分（logits）进行缩放，以维持梯度稳定**。若不进行缩放，当维度 $d_k$ 增大时，Q 和 K 的点积结果的方差也会增大，这会将 softmax 函数推入其梯度极小的饱和区域。这种情况会导致梯度消失，使模型训练变得极其困难甚至停滞。\n",
    "\n",
    "3. **在注意力操作的输出端添加一个线性变换（即输出投影矩阵 $W_o$），是为了将多个独立注意力头捕捉到的信息进行有效的融合与交互**。如果没有这个线性变换，直接堆叠注意力层意味着每个头的信息在后续层中将保持隔离，模型无法学习如何将这些来自不同子空间的信息组合成一个统一且更具表达力的表示，从而严重限制了模型的学习能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkfRjeETn2TU"
   },
   "source": [
    "# Transformer Decoder Block\n",
    "\n",
    "Transformer decoder layer consists of three modules: (1) self attention to process input sequence of vectors, (2) cross attention to process based on available context (i.e. image features in our case), (3) feedforward module to process each vector of the sequence independently. Complete the implementation of `TransformerDecoderLayer` in `cs231n/transformer_layers.py` and test it below. The relative error should be less than 1e-6.\n",
    "\n",
    "The Transformer decoder layer has three main components: (1) a self-attention module that processes the input sequence of vectors, (2) a cross-attention module that incorporates additional context (e.g., image features in our case), and (3) a feedforward module that independently processes each vector in the sequence. Complete the implementation of `TransformerDecoderLayer` in `cs231n/transformer_layers.py` and test it below. The relative error should be less than 1e-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JHyiLXjwpU4q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  1.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "\n",
    "N, T, TM, D = 1, 4, 5, 12\n",
    "\n",
    "decoder_layer = TransformerDecoderLayer(D, 2, 4*D)\n",
    "tgt = torch.randn(N, T, D)\n",
    "memory = torch.randn(N, TM, D)\n",
    "tgt_mask = torch.randn(T, T) < 0.5\n",
    "\n",
    "output = decoder_layer(tgt, memory, tgt_mask)\n",
    "\n",
    "expected_output = np.asarray([\n",
    "    [[ 1.1464597, -0.32541496,  0.39171425, -0.39425734,  0.62471056,\n",
    "      -1.8665842, -0.12977494, -1.6609063, -0.5620399,  0.45006236,\n",
    "       1.6086785,  0.7173523],\n",
    "     [-0.6703264,  0.34731007, -0.01452054, -0.0500976,  0.9617562,\n",
    "      -0.91788256,  0.5138556, -1.5247818,  2.0940537, -1.0386938,\n",
    "       1.0333964, -0.7340692],\n",
    "     [-1.1966342,  0.78882384,  0.1765188,  0.04164891,  1.9480462,\n",
    "      -0.94358695,  0.83423877, -0.44660965,  1.1469632, -1.6658922,\n",
    "      -0.27915588, -0.4043607],\n",
    "     [-0.96863323,  0.10736976, -0.18560877, -0.86474127, -0.12873,\n",
    "       0.36593518,  0.9634492, -0.9432319,  1.4652547,  1.2200648,\n",
    "       0.9218512, -1.9529796]]\n",
    "])\n",
    "print('error: ', rel_error(expected_output, output.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxBcIdRT7vvz"
   },
   "source": [
    "# Transformer for Image Captioning\n",
    "Now that you have implemented the previous layers, you can combine them to build a Transformer-based image captioning model. Open the file `cs231n/classifiers/transformer.py` and look at the `CaptioningTransformer` class.\n",
    "\n",
    "Implement the `forward` function of the class. After doing so, run the following to check your forward pass using a small test case; you should see error on the order of `e-5` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "W3Vxnysk72q6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores error:  1.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "\n",
    "N, D, W = 4, 20, 30\n",
    "word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n",
    "V = len(word_to_idx)\n",
    "T = 3\n",
    "\n",
    "transformer = CaptioningTransformer(\n",
    "    word_to_idx,\n",
    "    input_dim=D,\n",
    "    wordvec_dim=W,\n",
    "    num_heads=2,\n",
    "    num_layers=2,\n",
    "    max_length=30\n",
    ")\n",
    "\n",
    "features = torch.randn(N, D)\n",
    "captions = torch.randint(0, V, (N, T))\n",
    "\n",
    "scores = transformer(features, captions)\n",
    "expected_scores = np.asarray([\n",
    "    [[ 0.48119992, -0.24859881, -0.7489549 ],\n",
    "     [ 0.20380056,  0.08959456, -0.89954275],\n",
    "     [ 0.21135767, -0.17083111, -0.62508506]],\n",
    "\n",
    "    [[ 0.49413955, -0.50489324, -0.79341394],\n",
    "     [ 0.87452495, -0.4392967 , -1.1513498 ],\n",
    "     [ 0.2547267 , -0.26321974, -0.93643296]],\n",
    "\n",
    "    [[ 0.70437765, -0.5729916 , -0.7946507 ],\n",
    "     [ 0.18345363, -0.31752932, -1.7304884 ],\n",
    "     [ 0.61473167, -0.82634443, -1.2179294 ]],\n",
    "\n",
    "    [[ 0.5163983 , -0.7899667 , -1.0383208 ],\n",
    "     [ 0.28063023, -0.3603301 , -1.5435203 ],\n",
    "     [ 0.7222998 , -0.71457165, -0.76669186]]\n",
    "])\n",
    "\n",
    "print('scores error: ', rel_error(expected_scores, scores.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwUhSxEx7qyv"
   },
   "source": [
    "# Overfit Transformer Captioning Model on Small Data\n",
    "Run the following to overfit the Transformer-based captioning model on the same small dataset as we used for the RNN previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-HMqJq4T7qyv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base dir  D:\\study\\math-cs-ai\\cs231n\\assignment3\\cs231n\\datasets/coco_captioning\n",
      "(Iteration 1 / 200) loss: 5.048129\n",
      "(Iteration 11 / 200) loss: 2.808831\n",
      "(Iteration 21 / 200) loss: 2.009217\n",
      "(Iteration 31 / 200) loss: 1.627635\n",
      "(Iteration 41 / 200) loss: 1.297632\n",
      "(Iteration 51 / 200) loss: 1.165492\n",
      "(Iteration 61 / 200) loss: 0.813678\n",
      "(Iteration 71 / 200) loss: 0.641160\n",
      "(Iteration 81 / 200) loss: 0.467492\n",
      "(Iteration 91 / 200) loss: 0.409953\n",
      "(Iteration 101 / 200) loss: 0.307431\n",
      "(Iteration 111 / 200) loss: 0.124727\n",
      "(Iteration 121 / 200) loss: 0.119893\n",
      "(Iteration 131 / 200) loss: 0.090434\n",
      "(Iteration 141 / 200) loss: 0.059709\n",
      "(Iteration 151 / 200) loss: 0.047138\n",
      "(Iteration 161 / 200) loss: 0.028037\n",
      "(Iteration 171 / 200) loss: 0.022875\n",
      "(Iteration 181 / 200) loss: 0.017138\n",
      "(Iteration 191 / 200) loss: 0.016176\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0EAAAK7CAYAAAA0g4QSAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdrxJREFUeJzt3Xd8m+W9//+3JEvy3ntk70EmCQkjCZCwR4GyAoRdRnrIAX6ldEFL20D7paUts2VvelqglJ2Qwcggk+y97NiOYzveQ7J0//6QrcTYSWxHsiTr9XwcP45969atj9S7rt+5rutzmQzDMAQAAAAAYcIc6AIAAAAAoDsRggAAAACEFUIQAAAAgLBCCAIAAAAQVghBAAAAAMIKIQgAAABAWCEEAQAAAAgrhCAAAAAAYYUQBAAAACCsEIIAIIBMJlOHvhYtWnRCr/Pwww/LZDJ16bmLFi3ySQ2h9trHMnXqVI0YMeK45+3Zs0cmk0kvv/xyp67/5ptv6oknnuhacQCA44oIdAEAEM6WLl3a6udHHnlECxcu1IIFC1odHzZs2Am9zq233qpzzz23S88dO3asli5desI1hKOsrCwtXbpU/fv379Tz3nzzTW3YsEFz5szxT2EAEOYIQQAQQKecckqrn9PS0mQ2m9sc/766ujpFR0d3+HVyc3OVm5vbpRrj4+OPWw/aZ7fbg+azMwxDDQ0NioqKCnQpABBwTIcDgCDXMvXqyy+/1OTJkxUdHa2bb75ZkvTOO+9oxowZysrKUlRUlIYOHaqf/vSnqq2tbXWN9qbD9enTRxdeeKE+/fRTjR07VlFRURoyZIhefPHFVue1NyXtxhtvVGxsrHbs2KHzzz9fsbGxysvL03333afGxsZWzy8oKNAVV1yhuLg4JSYmaubMmVqxYkWXpom1+OCDDzRp0iRFR0crLi5O06dPbzOqdvDgQd1+++3Ky8uT3W5XWlqaTj31VM2fP997zpo1a3ThhRcqPT1ddrtd2dnZuuCCC1RQUNChOlasWKHTTz9d0dHR6tevnx599FG53W7v4+1NhzteXVOnTtVHH32kvXv3tpoS2aK8vFx33XWXcnJyZLPZ1K9fP/385z9v87mbTCbNnj1bzz77rIYOHSq73a6XX35ZAwcO1DnnnNPmvdTU1CghIUF33313h947AIQyRoIAIAQUFRXpuuuu009+8hP9/ve/l9ns+Tes7du36/zzz9ecOXMUExOjLVu26LHHHtO3337bZkpde7777jvdd999+ulPf6qMjAw9//zzuuWWWzRgwACdccYZx3yu0+nUxRdfrFtuuUX33XefvvzySz3yyCNKSEjQr371K0lSbW2tpk2bpvLycj322GMaMGCAPv30U1111VVd/izefPNNzZw5UzNmzNBbb72lxsZG/eEPf9DUqVP1xRdf6LTTTpMkXX/99Vq9erV+97vfadCgQaqoqNDq1atVVlbmrW369Onq27evnnrqKWVkZKi4uFgLFy5UdXX1cesoLi7WzJkzdd999+mhhx7Se++9pwcffFDZ2dm64YYbjvq849X19NNP6/bbb9fOnTv13nvvtXpuQ0ODpk2bpp07d+rXv/61TjrpJH311VeaO3eu1q5dq48++qjV+e+//76++uor/epXv1JmZqbS09PldDo1Z84cbd++XQMHDvSe++qrr6qqqooQBCA8GACAoDFr1iwjJiam1bEpU6YYkowvvvjimM91u92G0+k0Fi9ebEgyvvvuO+9jDz30kPH9X/m9e/c2IiMjjb1793qP1dfXG8nJycaPfvQj77GFCxcakoyFCxe2qlOS8c9//rPVNc8//3xj8ODB3p+feuopQ5LxySeftDrvRz/6kSHJeOmll475nr7/2i6Xy8jOzjZGjhxpuFwu73nV1dVGenq6MXnyZO+x2NhYY86cOUe99sqVKw1Jxvvvv3/MGtrT8p/J8uXLWx0fNmyYcc4553h/3r17d5v3eby6DMMwLrjgAqN3795tjj/77LPtfu6PPfaYIcn4/PPPvcckGQkJCUZ5eXmrc6uqqoy4uDjjnnvuaVP7tGnTjlkXAPQUTIcDgBCQlJSkM888s83xXbt26dprr1VmZqYsFousVqumTJkiSdq8efNxrzt69Gj16tXL+3NkZKQGDRqkvXv3Hve5JpNJF110UatjJ510UqvnLl68WHFxcW2aMlxzzTXHvX57tm7dqsLCQl1//fXe0TBJio2N1eWXX65ly5aprq5OkjRhwgS9/PLL+u1vf6tly5bJ6XS2utaAAQOUlJSkBx54QM8++6w2bdrUqVoyMzM1YcKEVse+//7bc7y6jmXBggWKiYnRFVdc0er4jTfeKEn64osvWh0/88wzlZSU1OpYXFycbrrpJr388sveaZMLFizQpk2bNHv27A7XAgChjBAEACEgKyurzbGamhqdfvrpWr58uX77299q0aJFWrFihd59911JUn19/XGvm5KS0uaY3W7v0HOjo6MVGRnZ5rkNDQ3en8vKypSRkdHmue0d64iWKWPtfR7Z2dlyu906dOiQJM96qVmzZun555/XpEmTlJycrBtuuEHFxcWSpISEBC1evFijR4/Wz372Mw0fPlzZ2dl66KGHOhRMuvrZHa+u473/zMzMNuu70tPTFRER4f18WrT3OUnSj3/8Y1VXV+uNN96QJD355JPKzc3VJZdcctwaAKAnIAQBQAhob4+fBQsWqLCwUC+++KJuvfVWnXHGGRo/frzi4uICUGH7UlJSdODAgTbHO/IH/9GuJ3nWSH1fYWGhzGazd+QjNTVVTzzxhPbs2aO9e/dq7ty5evfdd72jJpI0cuRIvf322yorK9PatWt11VVX6Te/+Y0ef/zxLtXXER2p62haPk/DMFodLykpUVNTk1JTU1sdP9reUAMGDNB5552np556Svn5+frggw90xx13yGKxdPl9AUAoIQQBQIhq+QPXbre3Ov7cc88Fopx2TZkyRdXV1frkk09aHX/77be7dL3BgwcrJydHb775ZqsgUFtbq3//+9/ejnHf16tXL82ePVvTp0/X6tWr2zxuMpk0atQo/fnPf1ZiYmK75/jD0eo62ojSWWedpZqaGr3//vutjr/66qvexzvqnnvu0bp16zRr1ixZLBbddtttXXsTABCC6A4HACFq8uTJSkpK0h133KGHHnpIVqtVb7zxhr777rtAl+Y1a9Ys/fnPf9Z1112n3/72txowYIA++eQTffbZZ5LUal1PR5jNZv3hD3/QzJkzdeGFF+pHP/qRGhsb9cc//lEVFRV69NFHJUmVlZWaNm2arr32Wg0ZMkRxcXFasWKFPv30U1122WWSpA8//FBPP/20Lr30UvXr10+GYejdd99VRUWFpk+f7tsPollH6pI8I1TvvvuunnnmGY0bN05ms1njx4/XDTfcoKeeekqzZs3Snj17NHLkSH399df6/e9/r/PPP19nn312h2uZPn26hg0bpoULF+q6665Tenq6P94yAAQlQhAAhKiUlBR99NFHuu+++3TdddcpJiZGl1xyid555x2NHTs20OVJkmJiYrRgwQLNmTNHP/nJT2QymTRjxgw9/fTTOv/885WYmNjpa1577bWKiYnR3LlzddVVV8liseiUU07RwoULNXnyZEmeBg8TJ07Ua6+9pj179sjpdKpXr1564IEH9JOf/ESSNHDgQCUmJuoPf/iDCgsLZbPZNHjwYL388suaNWuWLz8Gr47UJXlGaTZu3Kif/exnqqyslGEYMgxDkZGRWrhwoX7+85/rj3/8ow4ePKicnBzdf//9euihhzpdz5VXXqmHH36YhggAwo7J+P7EYgAA/Oz3v/+9fvGLX2jfvn3Kzc0NdDlha/z48TKZTFqxYkWgSwGAbsVIEADAr5588klJ0pAhQ+R0OrVgwQL99a9/1XXXXUcACoCqqipt2LBBH374oVatWtVmQ1YACAeEIACAX0VHR+vPf/6z9uzZo8bGRu/0r1/84heBLi0srV69WtOmTVNKSooeeughXXrppYEuCQC6HdPhAAAAAIQVWmQDAAAACCuEIAAAAABhhRAEAAAAIKyEdGMEt9utwsJCxcXFeXdOBwAAABB+DMNQdXW1srOzj7sZd0iHoMLCQuXl5QW6DAAAAABBIj8//7hbMIR0CIqLi5PkeaPx8fEBrgYAAABAoFRVVSkvL8+bEY4lpENQyxS4+Ph4QhAAAACADi2ToTECAAAAgLBCCAIAAAAQVghBAAAAAMIKIQgAAABAWCEEAQAAAAgrhCAAAAAAYYUQBAAAACCsEIIAAAAAhBVCEAAAAICwQggCAAAAEFYIQQAAAADCCiEIAAAAQFghBAEAAAAIK4QgAAAAAGGFEAQAAAAgrBCCAAAAAIQVQhAAAACAsEIIAgAAABBWCEEAAAAAwgohCAAAAEBYIQQBAAAACCuEIAAAAABhhRAEAAAAIKxEBLqAnuK7/AoVVtRrWHa8eqfEBLocAAAAAEfBSJCPPLt4p+58Y7W+3HYw0KUAAAAAOAZCkI/YIjwfZWOTO8CVAAAAADiWgIaghx9+WCaTqdVXZmZmIEvqMpvF81E6XIQgAAAAIJgFfE3Q8OHDNX/+fO/PFoslgNV0nXckyEkIAgAAAIJZwENQREREyI7+HKklBDESBAAAAAS3gK8J2r59u7Kzs9W3b19dffXV2rVr11HPbWxsVFVVVauvYGGP8IxgOVgTBAAAAAS1gIagiRMn6tVXX9Vnn32mf/zjHyouLtbkyZNVVlbW7vlz585VQkKC9ysvL6+bKz4670gQIQgAAAAIagENQeedd54uv/xyjRw5UmeffbY++ugjSdIrr7zS7vkPPvigKisrvV/5+fndWe4x2QlBAAAAQEgI+JqgI8XExGjkyJHavn17u4/b7XbZ7fZurqpj6A4HAAAAhIaArwk6UmNjozZv3qysrKxAl9Jph/cJcgW4EgAAAADHEtAQdP/992vx4sXavXu3li9friuuuEJVVVWaNWtWIMvqEtYEAQAAAKEhoNPhCgoKdM0116i0tFRpaWk65ZRTtGzZMvXu3TuQZXWJ3TsSRAgCAAAAgllAQ9Dbb78dyJf3KUaCAAAAgNAQVGuCQhmNEQAAAIDQQAjyEUaCAAAAgNBACPIRG2uCAAAAgJBACPIRe4RFEiNBAAAAQLAjBPmInelwAAAAQEggBPmId00QjREAAACAoEYI8hFvdzhGggAAAICgRgjykcONEVwBrgQAAADAsRCCfKQlBDldhtxuI8DVAAAAADgaQpCPtDRGkFgXBAAAAAQzQpCP2AhBAAAAQEggBPlIS2MEieYIAAAAQDAjBPmIyWSiQxwAAAAQAghBPnS4QxwhCAAAAAhWhCAf8m6YSggCAAAAghYhyIfshCAAAAAg6BGCfMg7EuRiw1QAAAAgWBGCfKilMQJrggAAAIDgRQjyIdYEAQAAAMGPEORDdIcDAAAAgh8hyIdojAAAAAAEP0KQD9kiLJIIQQAAAEAwIwT5UEtjBIeLEAQAAAAEK0KQDzEdDgAAAAh+hCAfOtwYgX2CAAAAgGBFCPIh73Q4RoIAAACAoEUI8iG7lRAEAAAABDtCkA+1jAQ10hgBAAAACFqEIB+y0RgBAAAACHqEIB8iBAEAAADBjxDkQ4e7wxGCAAAAgGBFCPIhusMBAAAAwY8Q5EN2q0USIQgAAAAIZoQgH7K3jATRHQ4AAAAIWoQgH6IxAgAAABD8CEE+RAgCAAAAgh8hyIe8m6U2uQJcCQAAAICjIQT5EC2yAQAAgOBHCPIhewSNEQAAAIBgRwjyIdYEAQAAAMGPEORDhCAAAAAg+BGCfMjOmiAAAAAg6BGCfMhmsUhiJAgAAAAIZoQgH7JbaYwAAAAABDtCkA+17BPkchtyuY0AVwMAAACgPYQgH2ppjCAxJQ4AAAAIVoQgHyIEAQAAAMGPEORDEWaTTCbP941NrsAWAwAAAKBdhCAfMplM3nVBtMkGAAAAghMhyMda9gqiQxwAAAAQnAhBPmaLYK8gAAAAIJgRgnzMOxJECAIAAACCEiHIx2xMhwMAAACCGiHIx7yNEZyEIAAAACAYEYJ87PBIEC2yAQAAgGBECPIx1gQBAAAAwY0Q5GMtI0HsEwQAAAAEJ0KQj9kYCQIAAACCGiHIx7yNEQhBAAAAQFAiBPkYI0EAAABAcCME+Rj7BAEAAADBjRDkY/YIiyRGggAAAIBgRQjyMVpkAwAAAMGNEORjTIcDAAAAghshyMe83eGcrgBXAgAAAKA9hCAfYyQIAAAACG6EIB9rWRPEPkEAAABAcCIE+Rj7BAEAAADBjRDkY4QgAAAAILgRgnyspTECa4IAAACA4EQI8rGWkaBGJyEIAAAACEaEIB+z0x0OAAAACGqEIB+zR1gksSYIAAAACFaEIB+jMQIAAAAQ3AhBPsZmqQAAAEBwIwT5WEt3uEanK8CVAAAAAGgPIcjHGAkCAAAAghshyMe8LbJZEwQAAAAEJUKQj9lpjAAAAAAENUKQjx05Hc4wjABXAwAAAOD7CEE+Zrd49gkyDKnJTQgCAAAAgg0hyMdaRoIk1gUBAAAAwYgQ5GNHhiDWBQEAAADBhxDkYxazSRazSRIhCAAAAAhGhCA/oEMcAAAAELwIQX5wuEOcK8CVAAAAAPg+QpAf2CxsmAoAAAAEK0KQH7SMBBGCAAAAgOBDCPIDG2uCAAAAgKBFCPIDe4Rnw1RCEAAAABB8CEF+wEgQAAAAELwIQX5gt7R0hyMEAQAAAMGGEOQHhxsj0CIbAAAACDaEID9gOhwAAAAQvAhBftCyTxAhCAAAAAg+hCA/sFvZJwgAAAAIVoQgP7DRGAEAAAAIWoQgP2BNEAAAABC8CEF+cLg7HCEIAAAACDaEID9gJAgAAAAIXkETgubOnSuTyaQ5c+YEupQTZqc7HAAAABC0giIErVixQn//+9910kknBboUn7BbLZIIQQAAAEAwCngIqqmp0cyZM/WPf/xDSUlJgS7HJ+gOBwAAAASvgIegu+++WxdccIHOPvvs457b2NioqqqqVl/BiDVBAAAAQPCKCOSLv/3221q9erVWrFjRofPnzp2rX//6136u6sQd7g7nCnAlAAAAAL4vYCNB+fn5uueee/T6668rMjKyQ8958MEHVVlZ6f3Kz8/3c5Vd0zIdjhbZAAAAQPAJ2EjQqlWrVFJSonHjxnmPuVwuffnll3ryySfV2Ngoi8XS6jl2u112u727S+00psMBAAAAwStgIeiss87S+vXrWx276aabNGTIED3wwANtAlAosUfQGAEAAAAIVgELQXFxcRoxYkSrYzExMUpJSWlzPNQwEgQAAAAEr4B3h+uJDjdGIAQBAAAAwSag3eG+b9GiRYEuwSfsjAQBAAAAQYuRID+wNa9nIgQBAAAAwYcQ5Ad2K40RAAAAgGBFCPKDln2CGAkCAAAAgg8hyA+ibZ7pcHWOJrndRoCrAQAAAHAkQpAfxEdZJUluQ6pxNAW4GgAAAABHIgT5QaTV4u0QV1nnDHA1AAAAAI5ECPKThObRoMp6QhAAAAAQTAhBfpIYTQgCAAAAghEhyE8YCQIAAACCEyHITwhBAAAAQHAiBPlJS4e4ChojAAAAAEGFEOQnjAQBAAAAwYkQ5CeEIAAAACA4EYL8JLE5BFURggAAAICgQgjykwRaZAMAAABBiRDkJ0yHAwAAAIITIchPWkJQRb0jwJUAAAAAOBIhyE+8I0G0yAYAAACCCiHIT1r2CapubJLbbQS4GgAAAAAtCEF+0jISZBhSdUNTgKsBAAAA0IIQ5Cf2CIuirBZJNEcAAAAAggkhyI9ojgAAAAAEH0KQH9EmGwAAAAg+hCA/IgQBAAAAwYcQ5EcJ0YQgAAAAINgQgvyIkSAAAAAg+BCC/IgQBAAAAAQfQpAfeUNQHSEIAAAACBaEID9iJAgAAAAIPoQgPyIEAQAAAMGHEORHdIcDAAAAgg8hyI8YCQIAAACCDyHIj2iMAAAAAAQfQpAftYSg6sYmudxGgKsBAAAAIBGC/KolBElSFVPiAAAAgKBACPIjq8WsaJtFEuuCAAAAgGBBCPKzRJojAAAAAEGFEORn8YQgAAAAIKgQgvysZV1QBSEIAAAACAqEID9jryAAAAAguBCC/KwlBNEdDgAAAAgOhCA/S4xmJAgAAAAIJoQgP/NOh6sjBAEAAADBgBDkZ4cbIzgCXAkAAAAAiRDkd7TIBgAAAIILIcjPDneHawpwJQAAAAAkQpDf0R0OAAAACC6EID9LjLZJYjocAAAAECwIQX7WMhJU09gkp8sd4GoAAAAAEIL8LD4ywvs9U+IAAACAwCME+VmExaxYuycIMSUOAAAACDxCUDdIoE02AAAAEDQIQd2AvYIAAACA4EEI6gaJhCAAAAAgaBCCugHT4QAAAIDgQQjqBi0hqLTGEeBKAAAAABCCusGI3ARJ0vxNBwJcCQAAAABCUDe4cGSWrBaTNhVVaWtxdaDLAQAAAMIaIagbJMXYNHVwuiTpvTX7A1wNAAAAEN4IQd3kB2NyJEn/WbtfbrcR4GoAAACA8EUI6iZnDklXXGSEiiobtGxXWaDLAQAAAMIWIaibRFotuvCkLEnSu0yJAwAAAAKGENSNfjAmV5L06YZi1TtcAa4GAAAACE+EoG40vneScpOiVNPYpHmbaZcNAAAABAIhqBuZzSZvg4T3VhcEuBoAAAAgPBGCutmlzSHoy+2lKqqsD3A1AAAAQPghBHWz/mmxmtA3WS63odeW7g10OQAAAEDYIQQFwM2n9pEkvfXtPjU4aZAAAAAAdCdCUABMH5apnMQoHapz6n3aZQMAAADdihAUABazSbMm95YkvfTNHhmGEeCKAAAAgPBBCAqQq8b3UpTVoq0HqrV0Z1mgywEAAADCBiEoQBKirbpinGfz1Be/2RPYYgAAAIAwQggKoBubGyR8seWA9pbVBrYYAAAAIEwQggKof1qspgxKk2FI/1yZH+hyAAAAgLBACAqwGcMzJEmbCqsCXAkAAAAQHghBAdYvNVaStPMg0+EAAACA7kAICrD+6TGSpIJDdWycCgAAAHQDQlCApcXaFRcZIbch7S2rC3Q5AAAAQI9HCAowk8mkfmktU+JqAlwNAAAA0PMRgoJA/zTPlLhdhCAAAADA7whBQaB/Gs0RAAAAgO5CCAoC/ZkOBwAAAHQbQlAQODwdrlaGYQS4GgAAAKBnIwQFgV4p0bKYTappbFJJdWOgywEAAAB6NEJQELBHWNQrOVqStLOEKXEAAACAPxGCgkS/VM+UuJ2lNEcAAAAA/IkQFCT6pzc3R2AkCAAAAPArQlCQaGmOQIc4AAAAwL8IQUGiX3Ob7F3sFQQAAAD4FSEoSLTsFbS/ol71DleAqwEAAAB6LkJQkEiOsSkp2ipJ2lXKlDgAAADAXwhBQaQ/U+IAAAAAvyMEBZF+NEcAAAAA/I4QFERaRoJ2MhIEAAAA+A0hKIgcng7HSBAAAADgL4SgINIyHW7XwVq53UaAqwEAAAB6JkJQEOmVHC2rxaR6p0sFh+oDXQ4AAADQIxGCgkiExawROQmSpG/3lAe4GgAAAKBnIgQFmYl9UyRJy3aVBbgSAAAAoGciBAWZU/olS5KW7yYEAQAAAP5ACAoy4/sky2I2Kb+8XvsrWBcEAAAA+FpAQ9Azzzyjk046SfHx8YqPj9ekSZP0ySefBLKkgIu1R3jXBS1nShwAAADgcwENQbm5uXr00Ue1cuVKrVy5UmeeeaYuueQSbdy4MZBlBVzLlDjWBQEAAAC+F9AQdNFFF+n888/XoEGDNGjQIP3ud79TbGysli1b1u75jY2NqqqqavXVE53S3Bxh+W46xAEAAAC+FjRrglwul95++23V1tZq0qRJ7Z4zd+5cJSQkeL/y8vK6ucruMb5PkswmaW9ZnYoqWRcEAAAA+FLAQ9D69esVGxsru92uO+64Q++9956GDRvW7rkPPvigKisrvV/5+fndXG33iIu0aqR3XRCjQQAAAIAvBTwEDR48WGvXrtWyZct05513atasWdq0aVO759rtdm8ThZavnmpiv+PvF+R2GzIMo7tKAgAAAHqEgIcgm82mAQMGaPz48Zo7d65GjRqlv/zlL4EuK+AO7xfU/kjQzoM1GvnwZ3r0ky3dWRYAAAAQ8gIegr7PMAw1NjYGuoyAG98nWWaTtLu0VgeqGto8vmjrQdU6XHrxm90qqW77OAAAAID2BTQE/exnP9NXX32lPXv2aP369fr5z3+uRYsWaebMmYEsKyjER1o1PNuzLqi9KXE7D9ZIkpwuQ28u39ettQEAAAChLKAh6MCBA7r++us1ePBgnXXWWVq+fLk+/fRTTZ8+PZBlBY3D+wW1nRK3o6TG+/0by/fJ0eTutroAAACAUBYRyBd/4YUXAvnyQW9MryRJu7WpsLLNY7uaR4JsFrMOVjfqkw1FumR0TjdXCAAAAISeoFsThMOGZnm63209UC2X+3AXuIo6h0prHJKkm07rI0l66Zs93V0eAAAAEJIIQUGsV3K0oqwWNTjd2lNW6z3esh4oOyFSt57WTzaLWWvzK7Rm36FAlQoAAACEDEJQELOYTRqUGSdJ2lJU7T2+s8QTiPqnxyotzq4LR2VJkl5ZsqfbawQAAABCDSEoyA3L8oSgzUVV3mM7mkeC+qfFSpJumtxXkvTR+iLaZQMAAADHQQgKckMyPeuCthQfDkE7mzvD9U/3hKCRuQka0ytRTpehd1fv7/4iAQAAgBBCCApyLc0RNh85Ha55JGhA80iQJF01Pk+S9K9VBTIMQwAAAADaRwgKcoOb1wTtr6hXZb1TDU6X9pXXSZL6p8d4zzv/pCxFWs3aUVKj7wrattQGAAAA4EEICnIJUVblJEZJkrYWV2tvWZ3chhQXGaG0WLv3vPhIq84dnilJ+teq/IDUCgAAAIQCQlAIGNrcHGFLcZV2NK8HGpAeK5PJ1Oq8K8Z5psR9sLZQDU5X9xYJAAAAhAhCUAhoaY6wuajKux6o/xHrgVpM7p+i7IRIVTU0af7mA91aIwAAABAqCEEhYIi3TXb1MUOQ2WzS5eNyJXkaJAAAAABoixAUAlo6xG0trta2Ay0hKKbdcy8f6wlBX247qANV7BkEAAAAfB8hKAT0SYmRPcKseqfLu2nqgPS2I0GS1Cc1Rif3SZLbkN5bw55BAAAAwPcRgkKAxWzytsqWJKvFpLzk6KOef0XzlLj/rC30e20AAABAqCEEhYihzc0RJKl3SoyslqP/R3fW0AxJnkYKh2odfq8NAAAACCWEoBDR0hxBkga00xThSKmxdg1sni63fHe5X+sCAAAAQg0hKEQMOWIkqH96+00RjjSxX7IkafnuMr/VBAAAAIQiQlCIGHrESFB77bG/b2LfFEnS8l2MBAEAAABHIgSFiMRom/qkeJohDMuOP87Zh0eCNhdXqbLO6dfaAAAAgFBCCAohz14/Ts9eN7bV1LijSY+LVL+0GBmG9O0eRoMAAACAFoSgEDIkM17njsjq8PmHp8SxLggAAABoQQjqwU5pnhK3jOYIAAAAgBchqAc7pZ9nJGhTYZUq61kXBAAAAEiEoB4tIz5SfVKi5TaklawLAgAAACQRgnq8ltEgNk0FAAAAPAhBPZx301SaIwAAAACSpIhAFwD/aukQt35/pTYWVsrtlmodTYqLjFDvlBjF2rkFAAAAEF74C7iHy06MUq/kaO0rr9MFf/26zePJMTYNzojTH644SXnJ0QGoEAAAAOheTIcLAzdM6q0oq0XxkRHKSohUv9QYJcfYJEnltQ4t3VWmj9YXBbhKAAAAoHswEhQGbj29n249vV+b41UNTj3y3036v1UFOlTnCEBlAAAAQPdjJCiMxUda1at5ClxlHfsIAQAAIDwQgsJcYrRVkthMFQAAAGGDEBTm4qM8IaiCkSAAAACEiS6FoPz8fBUUFHh//vbbbzVnzhz9/e9/91lh6B6J0Z4GCRWMBAEAACBMdCkEXXvttVq4cKEkqbi4WNOnT9e3336rn/3sZ/rNb37j0wLhX4nNI0GVNEYAAABAmOhSCNqwYYMmTJggSfrnP/+pESNGaMmSJXrzzTf18ssv+7I++FlCFGuCAAAAEF66FIKcTqfsdrskaf78+br44oslSUOGDFFREfvNhJKWxgi1DpccTe4AVwMAAAD4X5dC0PDhw/Xss8/qq6++0rx583TuuedKkgoLC5WSkuLTAuFfcZFWmUye7xkNAgAAQDjoUgh67LHH9Nxzz2nq1Km65pprNGrUKEnSBx984J0mh9BgMZsUH9kyJY51QQAAAOj5IrrypKlTp6q0tFRVVVVKSkryHr/99tsVHR3ts+LQPRKirKqsdzISBAAAgLDQpZGg+vp6NTY2egPQ3r179cQTT2jr1q1KT0/3aYHwv5Z1QewVBAAAgHDQpRB0ySWX6NVXX5UkVVRUaOLEiXr88cd16aWX6plnnvFpgfC/BDZMBQAAQBjpUghavXq1Tj/9dEnSv/71L2VkZGjv3r169dVX9de//tWnBcL/2DAVAAAA4aRLIaiurk5xcXGSpM8//1yXXXaZzGazTjnlFO3du9enBcL/EqI8S8NYEwQAAIBw0KUQNGDAAL3//vvKz8/XZ599phkzZkiSSkpKFB8f79MC4X+JUZ6RoMo6usMBAACg5+tSCPrVr36l+++/X3369NGECRM0adIkSZ5RoTFjxvi0QPiftzECI0EAAAAIA11qkX3FFVfotNNOU1FRkXePIEk666yz9IMf/MBnxaF70BgBAAAA4aRLIUiSMjMzlZmZqYKCAplMJuXk5LBRaohqCUGsCQIAAEA46NJ0OLfbrd/85jdKSEhQ79691atXLyUmJuqRRx6R2+32dY3ws5bucIQgAAAAhIMujQT9/Oc/1wsvvKBHH31Up556qgzD0DfffKOHH35YDQ0N+t3vfufrOuFHhzdLpTECAAAAer4uhaBXXnlFzz//vC6++GLvsVGjRiknJ0d33XUXISjEJB4xHc7tNmQ2mwJcEQAAAOA/XZoOV15eriFDhrQ5PmTIEJWXl59wUehe8c0hyG1INY6mAFcDAAAA+FeXQtCoUaP05JNPtjn+5JNP6qSTTjrhotC9Iq0WRVo9t0IlHeIAAADQw3VpOtwf/vAHXXDBBZo/f74mTZokk8mkJUuWKD8/Xx9//LGva0Q3SIyyqdjZoIo6p/KSA10NAAAA4D9dGgmaMmWKtm3bph/84AeqqKhQeXm5LrvsMm3cuFEvvfSSr2tEN/DuFVRPcwQAAAD0bF3eJyg7O7tNA4TvvvtOr7zyil588cUTLgzdKyGavYIAAAAQHro0EoSep6VDXMURa4KW7CjVjD8v1re7aXYBAACAnoMQBEmH9wo6ciToX6sKtO1Ajd5fuz9QZQEAAAA+RwiCpCPWBB2xYeqOgzWSpD2ltQGpCQAAAPCHTq0Juuyyy475eEVFxYnUggBKjLZJOjwSZBiGdpYQggAAANDzdCoEJSQkHPfxG2644YQKQmAkfG9NUHFVg2odLklSYWWDGpwuRVotAasPAAAA8JVOhSDaX/dcLWuCKppHgnY0jwK12Fdep0EZcd1eFwAAAOBrrAmCpMMjQZV17Yeg3UyJAwAAQA9BCIIkKTGq9ZqgnQdbh6C9ZYQgAAAA9AyEIEg6cjqcpztcy0hQTmKUJGl3aV1gCgMAAAB8jBAESVJCcwhqcLrV4HRpR4ln5Gf6sAxJdIgDAABAz0EIgiQp1hYhs8nz/b7yOpXWNEqSzhySLknaw3Q4AAAA9BCEIEiSzGaTtznC6r2HJEmZ8ZEakeNpi17U3CYbAAAACHWEIHi1bJi6sjkEDUiPVVK0VXGRnk7qe8tYFwQAAIDQRwiCV8tI0KojQpDJZFLf1BhJtMkGAABAz0AIgldLCGoJO/3TYyVJfVI8IYg22QAAAOgJCEHwammT3WJAWnMIah4JojkCAAAAegJCELwSo1qHoP7pnvDTJyVaEtPhAAAA0DMQguCVcEQIio+MUFqsXdIRI0FsmAoAAIAegBAEr4Tm7nDS4aYIktS3eU1QcVWD6h20yQYAAEBoIwTB68jpcAOamyJInrVC8S1tssuZEgcAAIDQRgiC15GNEfqnHQ5BR7bJ3sO6IAAAAIQ4QhC8Eo4yEiQd2SGOdUEAAAAIbYQgeB05EtQmBKUwEgQAAICegRAEr/T4SNkizEqOsSk3KbrVY31Sj90m+0/ztul/3lojR5Pb73UCAAAAJyIi0AUgeMRHWvWvOyYp2maRxWxq9Zh3JKidDVNLqhr01y+2S5IuHpWts4dldLmGlXvKlZccrYz4yC5fAwAAADgWRoLQykm5iRqQHtfmeEtjhANVjaqsc7Z6bN7mA97vP9tY3OXX3lJcpSueXao7Xl/V5WsAAAAAx0MIQockRts0OMMTjj7eUNTqsc82Hg5B8zcfUJOra1PiVuw5JEnafqCmi1UCAAAAx0cIQoddNjZHkvTu6gLvsaoGp5buLJUkRVrNOlTn9IaZztq4v1KSVNPYpKoG53HOBgAAALqGEIQOu3RMjswmz4jN3ua1QQu3lMjpMjQgPVYXnpQtqetT4jYUVnq/L65sOPGCAQAAgHYQgtBhGfGROnVAqiTp3dX7JUmfN0+FmzEsQ+cMz2w+VizDMDp1bUeTW9uKD0+DK6yo90XJAAAAQBuEIHTKFeNyJUnvrilQg9OlRVtLJEnnDM/U6QNTFW2zqLCyQev3Vx7rMm1sL6mW44i1RIwEAQAAwF8IQeiUGcMyFWOzKL+8Xn+ev021Dpcy4yN1Um6CIq0WTRmUJqnzU+I2Fla1+rmIEAQAAAA/IQShU6JsFp0/MkuS9Pcvd0mSZgzPkMnk2VeoZUrckR3jOqKlKYLV4rlOUSXT4QAAAOAfhCB02uXNU+Jalv20BB9JmjYkXRFmk3aU1GjnwY63ut7QPBJ0Sr8USYwEAQAAwH8IQei0CX2SlZMYJUlKiLJqQt9k72MJUVZN6u8JMh2dEudyG9pc5AlBZw/NkEQIAgAAgP8QgtBpZrNJPxzvGQ06d3imrJbWt1Fnp8TtLq1VncOlKKtFpw7wBCgaIwAAAMBfCEHokrunDdBfrxmjn184tM1jM4ZlyGSSvsuv6FCY2di8P9DQrDhlN48wsWEqAAAA/IUQhC6xWsy6eFS24iOtbR5Lj4/UmLxESdLnm44/Ja6lM9zw7ARF2yKUEOW5JqNBAAAA8AdCEPzi3BEtU+KOH4I2NHeGG5ETL0nKSoiUxLogAAAA+AchCH7Rsi5o2a5yVdQ5jnqeYRitRoKkI0JQBW2yAQAA4HuEIPhF75QYDcmMk8ttaP7mkqOeV3CoXpX1TlktJg3KiJMkZTWvC2IkCAAAAP5ACILfzBh+/ClxLU0RBmXEyRbhuR2z4lumwzESBAAAAN8jBMFvzhnu2fPny20HVedoaveclqlwI5qnwklSJmuCAAAA4EeEIPjNsKx45SZFqbHJrS+3HWz3nLX5FZKk4c1NESR522QTggAAAOAPAQ1Bc+fO1cknn6y4uDilp6fr0ksv1datWwNZEnzIZDIdc+PU/PI6fbOjVJI0uX+K93jLSBAtsgEAAOAPAQ1Bixcv1t13361ly5Zp3rx5ampq0owZM1RbWxvIsuBDLSHoi80H5HS5Wz328pI9chvS6QNTNSA9znu8pTtcTWOTqtkwFQAAAD4WEcgX//TTT1v9/NJLLyk9PV2rVq3SGWecEaCq4EvjeicpNdam0hqH3l6Rr+tP6S1Jqm5w6p0V+ZKkm0/r2+o5LRumVtY7VVTZoLh2NmQFAAAAuiqo1gRVVno6hSUnJ7f7eGNjo6qqqlp9IbhZzCbdPW2AJOmxT7Z4O779c2WBahqb1D8tRlMGprV5HhumAgAAwF+CJgQZhqF7771Xp512mkaMGNHuOXPnzlVCQoL3Ky8vr5urRFfcMKmPxvZKVE1jk37+3ga53IZeXrJbkmcUyGw2tXkOG6YCAADAX4ImBM2ePVvr1q3TW2+9ddRzHnzwQVVWVnq/8vPzu7FCdJXFbNJjl58km8WsBVtKdO8/1yq/vF6J0VZdNia33edkJtAhDgAAAP4RFCHoxz/+sT744AMtXLhQubnt/1EsSXa7XfHx8a2+EBoGZsTpx2d6psX9Z22hJOnaCb0UZbO0e352AhumAgAAwD8CGoIMw9Ds2bP17rvvasGCBerbt+/xn4SQdcfU/hqS6ekCF2E26YZJfY56LhumAgAAwF8CGoLuvvtuvf7663rzzTcVFxen4uJiFRcXq76ef/3viawWs/7fD0cpNdamm07t4w067clqng7HXkEAAADwNZNhGEbAXtzUdkG85GmVfeONNx73+VVVVUpISFBlZSVT43qYnQdrdNbjixVrj9CGX58T6HIAAAAQ5DqTDQK6T1AA8xeC3Pc3TGWvIAAAAPhKUDRGAL6vZcNUiXVBAAAA8C1CEIJWy2jQZxuKtWF/pSrrnQGuCAAAAD1BQKfDAceSmxSlLcXVenzeNj0+b5sk6dzhmXr2+nEBrgwAAAChjBCEoDXn7EGKskVoX3mdCsrrVFbr0Kcbi7WjpEYD0mNbnfvPlfmKslp00ajsAFULAACAUEEIQtAakZOgv10zxvvzzS+v0IItJfrP2v26b8Zg7/EN+yv1k3+tU4TZpLOHZhx1A1YAAABAYk0QQsgloz2jPP9ZW9iqs+Aby/dKkprchvZX1AWkNgAAAIQOQhBCxvRhGYq2WbSvvE5r8iskSVUNTr2/ptB7Tn45G+0CAADg2AhBCBnRtgjNGJYhSfpgrSf4vLd6v+qdLu85+YcYCQIAAMCxEYIQUi4ZnSNJ+nBdoZwut15f5pkKlxTt2VMov5wQBAAAgGMjBCGknDYwVckxNpXWOPTE/G3aXlKjKKtFt5zWVxLT4QAAAHB8hCCEFKvFrAtGZkmSnlq4U5J06ZhsDc2Kl8R0OAAAABwfIQgh59IxrfcCmjmxt/KSoyUxHQ4AAADHRwhCyBnbK0m5SVGSpFF5iRqRk+D9uaqhSZX1zkCWBwAAgCBHCELIMZlMuu30fjKbpB9PGyDJ0zkuNdYmidEgAAAAHFtEoAsAuuKGSb01c2IvRVgO5/jcpGiV1jhUcKhOI3ISAlgdAAAAghkjQQhJJpOpVQCSdMS6IDrEAQAA4OgIQegx8prXBdEhDgAAAMdCCEKP0ZUOca8t26sH310vt9vwV1kAAAAIMoQg9Bh5Sc0h6FDHpsM5mtz67Yeb9Na3+7SxsMqfpQEAACCIEILQY+Qle6bDFRyqk2Ecf2RnU1GVGpvckqSS6ga/1gYAAIDgQQhCj5GdGCWzSWpwunWwpvG456/ae8j7fUn18c8HAABAz0AIQo9htZiVldDcHKEDHeJWHxmCqghBAAAA4YIQhB4lN+nwlLjjaT0SxHQ4AACAcEEIQo/S0Q5xhRX1Kq46HHwOMh0OAAAgbBCC0KN4O8QdZzrckaNAEmuCAAAAwgkhCD1KS4e4422Y2hKCRuUmSGIkCAAAIJwQgtCjeKfDHRGC/vtdof65Ir/Veav3eULQuSOyJHlCUEfaagMAACD0EYLQo7RMhyusaFCTy61P1hfpx2+t0U/+vU5fby+VJNU7XNrUvDnqOcMzJEkOl1uV9c7AFA0AAIBuRQhCj5IeZ5ctwiyX29CX2w/qvv/7zvvY3E82y+02tK6gQk1uQ5nxkeqbGqOEKKsk1gUBAACEC0IQehSz2aTcRM+6oNlvrlGdw6UJfZIVa4/QxsIq/XddoVY1T4Ub1ztJJpNJaXF2SewVBAAAEC4IQehxcpvXBdU5XMpJjNKz14/THVP6SZL++NlWLd1ZJkka0ytRkmf0SJIO1rBXEAAAQDggBKHHyWveMNUeYdZz149TcoxNt5zWTxnxdhUcqtdXzWuDxvVOknQ4BDESBAAAEB4IQehxLh6VrQHpsfrzVaM1IsfTAjvKZtH/nj3Ie44twqzh2Z7HvNPhWBMEAAAQFghB6HEm9kvR/Hun6PyRWa2OXzEuVwPTYyV59geyRXhu//S4SEnsFQQAABAuCEEIGxEWsx65dIQy4u26+uRe3uPp8S0jQawJAgAACAcRgS4A6E6n9EvR8p+d3eoY0+EAAADCCyNBCHve7nA0RgAAAAgLhCCEvbTmNUHVjU2qd7gCXA0AAAD8jRCEsBcfGSF7c5MEmiMAAAD0fIQghD2TyURzBAAAgDBCCAIkpcU2rwvqwkhQg9OlP83bpi82H/B1WQAAAPADusMBOrxXUGc7xFU1OHXryyv17Z5ypcTYtPIXZ8tkMvmjRAAAAPgII0GAurZX0MHqRl393DJ9u6dcklRW61DBoXq/1AcAAADfIQQBOjwdrqSDbbILDtXpyueWalNRlVJjbcpNipIkrc2v8FeJAAAA8BFCEKDDI0EHa44fgjbsr9RlTy/R7tJa5SRG6f/umKxpg9MlSesKKvxZJgAAAHyANUGAjlgTdJyRoIVbS3T3G6tV53BpcEacXr75ZGUlROmk3ARJ0nf5lX6vFQAAACeGEARISotrWRPUfgg6VOvQf9bu1yMfbZbLbejUASl65rpxio+0SpJG5yVKktbvr1STy60IC4OsAAAAwYoQBEhKbw5B5bWNcrkNWcwmfbOjVM8u3qktxdWtWmdfPjZXcy8bKVvE4aDTLy1WMTaLah0u7ThYoyGZ8d3+HgAAANAx/HM1ICkl1i6zSXIbUllNow5WN+r2V1fqq+2l3gCUmxSlB84dov/3w5NaBSBJsphNGumdEldx1NdZX1Cpm176VntKa/32XgAAAHBsjAQB8oSYlFi7DlY3qqS6Uf+3Ml+1DpdG5MTrkUtGaGBGnGLtx/6vy6i8RC3bVa7vCip11cntn/OXL7Zr4daD6pu6V7+6aJgf3gkAAACOh5EgoFlLm+yVe8r15rf7JEk/O3+oxvRKOm4AkqRRuYmSjj4S5HIbWr67TJK0vaT6xAsGAABAlxCCgGYtbbIfn7dNTpehKYPSNLl/aoefP6q5OcLW4mo1OF1tHt9UWKXqhiZJ0o6SmhMvGAAAAF1CCAKatTRHqG5okskkPXDukE49PzshUqmxdjW5DW0srGrz+NJdpd7viyobVN3gPLGCAQAA0CWEIKBZy15BknTp6BwNy+5chzeTyaRRx2iOsHRnWauftzMaBAAAEBCEIKBZy15BNotZ904f1KVrtEyJW1dQ0eq40+XWt7vLJR0ecdpxgBAEAAAQCIQgoNnUwWnKTojU/ecMUl5ydJeu0RKCviuobHV8/f5K1TpcSoy26rwRmZJojgAAABAotMgGmvVOidGSB886oWuclOOZDre7tFaVdU4lRFslHZ4KN7FvsgZlxkmStjESBAAAEBCMBAE+lBRjU+8UzyjS2iOmxC3b5QlBk/qlaFCGJwTRIQ4AACAwCEGAj53cJ1mS9KfPt8rR5Jajya2Vew5JkiYPSNWAtFhJ0v6KetU0NgWsTgAAgHBFCAJ87N7pg5QQZdV3BZX6f59v1XcFFap3upQaa9PA9FglxdiU2rwxK6NBAAAA3Y8QBPhYdmKU/njFSZKkv3+5S3+et02SNLFfikwmkyRpUIZnNGj7AZojAAAAdDdCEOAHM4Znatak3pKkJTsPrwdqMTDdE4IYCQIAAOh+hCDATx48f6iGZh3ecHVS/8MhaEBzcwQ2TAUAAOh+hCDATyKtFj157RglRFk1NCte/VJjvI8Nah4J2uaH6XDvri7Q3W+sVr3D5fNrAwAA9ATsEwT4Uf+0WH31wDTZI8ze9UCSNLB5JKjgUL3qHE2Ktvnmv4qGYej3H29WaY1DM4Zn6JLROT65LgAAQE/CSBDgZ/GRVtkjLK2OJcfYlBJjkyTtLKn12WttO1Cj0hpH8/c0XQAAAGgPIQgIkIEZvp8S982OUu/3W4sJQQAAAO0hBAEBMjDd980RWjrRSdIWQhAAAEC7CEFAgLSMBO0oqZbLbWjFnnK9+PVuHap1dOl6TS63lu86HIIKDtWrprHJJ7UCAAD0JDRGAAKkZSRo6c4yTfjdfJU1h5+1+RX66zVjOn29DYVVqm5sUlxkhCKtFh2sbtS2A9Ua2yvJp3UDAACEOkaCgAAZ1DwSVOtwqazWobhIz79JfLy+SCVVDZ2+3pKdnvVAp/RL0ZBMT8BiXRAAAEBbhCAgQFJi7frtpSN086l99catE7X6l9M1tleimtyG3vo2v9PXW7LDMxXu1P6EIAAAgGNhOhwQQNed0rvVzzdM6qPV+9bqzW/36q5p/WW1tP/vFI4mt+qdLiVEWSVJjU0urdhTLkmaPCBV6woqJUlbiqv8WD0AAEBoYiQICCLnjcxUSoxNB6oaNW/TgaOeN+vFb3XK77/Q0uZucKv3Vqixya3UWLsGpse2GgkyDKNbagcAAAgVhCAgiNgjLLp6Qp4k6dWle9o9J7+8Tkt3lane6dLtr63U1uJqLW1eDzS5f4pMJpMGpMfKbJIO1Tl1sKaxu8oHAAAICYQgIMhcO7G3zCZp2a7ydjdS/WLz4RGi6oYm3fjSt/p0Y7Ek6dQBKZKkSKtFfVJiJLEuCAAA4PsIQUCQyUmM0vRhGZKk15bubfP4/M0lkqTZ0waof1qMiiobtO2AZ8PVyf1TvecNpjkCAABAuwhBQBC6YVIfSdK7qwtU1eD0Hq9qcGpZ84aol4/L1Ss3T1B6nF2SlJccpbzkaO+5LSFoCyEIAACgFUIQEIQm90/RwPRY1Tpcen3Z4dGgxVsPqsltaEB6rPqmxig3KVov3zRBo3ITdOeUAa2uMTjDE4Lam1IHAAAQzghBQBAymUy6Y0p/SdKLX+9Wg9MlSZrfvB7o7KEZ3nOHZcfrP7NP07UTe7W6RstI0LYD1XK56RAHAADQghAEBKmLR2crJzFKpTUO/d/KfDldbi3c4lkPNH1Y+nGf3zslRpFWsxqcbu0rr/N3uQAAACGDEAQEKavFrNvP6CdJeu7LXVq2q0xVDU1KibFpdF7ScZ9vMZs0ML2lOQKbpgIAALQgBAFB7MrxeUqJsangUL1+8f4GSdKZQ9JlMZs69PzDHeJq/FYjAABAqCEEAUEsymbRTaf2kSTtLfNMaTt7WMYxntFaS3OErQfajgTVOZp0x2urdNurK/WvVQWqqHOceMEAAAAhICLQBQA4tusn9dGzi3epprFJtgizTh+YevwnNRuWHS9J+np7qQ7VOpQUY/M+9vcvd3k3WZ236YAsZpNOHZCqxy4fqayEKN++CQAAgCDCSBAQ5BKirLrulN6SpDMGpira1vF/uzilX4qGZMapqqFJf56/zXu8pLpBf/9ylyTpsjE5GpIZJ5fb0JfbDuqNZft8+wYAAACCDCEICAFzzh6oX188XL+5ZESnnmcxm/Sri4ZJkt5Yvs+7Z9Cf521XncOl0XmJevzKUfp0zhl65FLPtZc2b8YKAADQUxGCgBAQabVo1uQ+yk7s/DS1yf1Tdc7wDLnchh75cJO2H6jWOys8oz0/v2CoTCZPk4Wpg9IkSd/lV6i2scl3xQMAAAQZQhAQBn52/lDZLGZ9tb1Ut726Um5DmjEsQyf3Sfaek5ccrZzEKDW5Da3ae6jV8w3D0P6KehkGm64CAIDQRwgCwkDvlBjddFofSdKesjpZzCY9cN6QNudN6p8iqe2UuBe+3q1TH12gf60q8HutAAAA/kYIAsLE7GkDlBprlyRdMyFP/dNi25wzqV9zCNp5OAS53YZe+maP5zjrhQAAQA9Ai2wgTMRFWvX0zLH6eH2R/nf6oHbPOaV5JGj9/krVNDYp1h6hZbvLtL+iXpK0r3mvIgAAgFDGSBAQRib0TdbDFw9XQpS13cdzEqPUKzlaLrehFXvKJanVFLg9hCAAANADEIIAtNIyJW7ZzjLVNjbp0w3F3sdKaxrpHAcAAEIeIQhAK6f093SMW7qrTJ9sKFadw6W+qTFKjPaMHu0rZzQIAACENkIQgFYm9UuVJG3YX6lXl+6RJF0+Nke9k6MlSXvLagNVGgAAgE8QggC0kpkQqb6pMXIb0rqCSplM0g/G5qp3SowkaS/rggAAQIgjBAFo45R+hzdRndQvRTmJUeqd4hkJojkCAAAIdYQgAG2c0twcQZIuH5srSerVPB1uXznT4QAAQGhjnyAAbUzun6pIq1mRVovOHZEpSeqTynQ4AADQMwR0JOjLL7/URRddpOzsbJlMJr3//vuBLAdAs7Q4u96981S9e+dkxdg9/1bS0hihsKJejiZ3IMsDAAA4IQENQbW1tRo1apSefPLJQJYBoB3DsuPVLy3W+3NanF1RVovchlRwiNEgAAAQugI6He68887TeeedF8gSAHSQyWRS75RobSmu1t7yulYBCQAAIJSEVGOExsZGVVVVtfoC0H1aOsTtLaU5AgAACF0hFYLmzp2rhIQE71deXl6gSwLCinevoHLfT4fbXFSll77ZLbfb8Pm1AQAAjhRSIejBBx9UZWWl9ys/Pz/QJQFhxTsS5OMOcY1NLt3y8gr9+r+bNH/zAZ9eGwAA4PtCqkW23W6X3W4PdBlA2Oqd3NImu/3pcNUNTr2yZI/qnS7NOXuQrJaO/TvLv1ftV2FlgyRp/f5KzRie6ZuCAQAA2hFSIQhAYLWMBOWX18vlNmQxmyR5RnLeXL5Pf1uwQ+W1DklSVX2THrl0xHGv6XS59dTCHd6fNxWy1g8AAPhXQENQTU2Nduw4/MfP7t27tXbtWiUnJ6tXr14BrAxAe7ISImW1mORwuVVc1aCcxChtP1Ctm19ZofzyeklSXnKUCg7V67VlezUoI1bXT+pzzGu+t3q/9lfUK8JsUpPb0KYiQhAAAPCvgK4JWrlypcaMGaMxY8ZIku69916NGTNGv/rVrwJZFoCjiLCYlZvUsi6oVoZh6IF/r1N+eb3S4uz6/Q9GauF9U/X/nTNYkvTwfzfpmx2lR71ek8utJ5tHge6eNkCSVFTZ4B1NAgAA8IeAhqCpU6fKMIw2Xy+//HIgywJwDC1T4vaV1emD7wq1el+Fom0W/Xf2abp2Yi9FWMy6c0p/XTYmRy63obveWK0txe2P7vxnbaH2ldcpJcamH03p5732ZkaDAACAH4VUdzgAgdc72RNUthRX69FPtkiS7praX5kJkd5zTCaTfn/ZSI3plajKeqfO/8tXuuftNdp+oFqS5HIb2lFS7R0Fuu2Mfoq2RWhYVrwkQhAAAPAvGiMA6JSWvYLeWL5XTpehnMQo3Xp6vzbnRVot+scN4/XAv9bpiy0l+s/aQn3wXaEGZ8RpT1mtGpxuSVJStFXXn9JbkjQsK16fbCimOQIAAPArQhCATmmZsuZ0eTY1/dn5QxVptbR7bmqsXS/ceLI27K/Ukwt26NONxdpS7BkNirJaNCQrTv/fjMGKsXt+FQ1tHgmiOQIAAPAnQhCATmkJQZI0oU+yzh95/D19RuQk6Nnrx2lHSY12lNRoUEaseqfEeFtstxiW7QlBO0pq1OB0HTVcAQAAnAhCEIBOyU2KVozNojqnS7+6aJhMJtPxn9RsQHqsBqTHHvXxrIRIJUZbVVHn1I6SGo3ISfBFyQAAAK0QggB0SqTVotdunagml+HzkGIymTQsK15LdpZpU1EVIQgAAPgF3eEAdNrYXkma0DfZL9du6RBHcwQAAOAvhCAAQYXmCAAAwN8IQQCCSktzhM1FVTIMI8DVAACAnogQBCCo9E+Llc1iVnVDkwoO1Qe6HAAA0AMRggAEFVuEWQMzPB3kOjIlbkdJtX774Sa9vmyv1hVUqLHJ5e8SAQBAiKM7HICgMzQrXhsLq7SpsErnDD/6PkT55XW65h/LdbC60XvMajHp7KEZ+uMPRynWzq84AADQFiNBAIJOS4e4BVtKtGhricpqGtucc6jWoVkvfauD1Y3qlxqjMwalKSnaKqfL0CcbijXz+eWqqHN0d+kAACAE8M+kAILOqLxESdL6/ZW68aUVkqS85CidNyJLl4zOVr/UWN3yygrtOlir7IRIvXnbKcpMiJRhGFq195BufXWlvsuv0NV/X6ZXb5mg9LjIAL4bAAAQbExGCLdfqqqqUkJCgiorKxUfHx/ocgD4iGEY+r9VBVqyo1Tr9ldq18HaVo8nRFlVWe9UfGSE/nXnZA3KiGv1+Nbial33gmeaXJ+UaL1+60TlJkV351sAAADdrDPZgBAEIOhVNTi1ZEep/rO2UF9sKZGjyS1bhFmv3zLxqJu27imt1cznl2t/Rb1SY+16YdZ47wgTAADoeQhBAHqsynqnFm0tUd/UGJ2Um3jMc4sq63XTSyu0pbhakVaznrhqjM4dcfRGCwAAIHR1JhvQGAFASEmIsuqS0TnHDUCSlJUQpX/dOVlTB6epwenWnW+s0otf7/Z/kQAAIKgRggD0aLH2CD1/w3hdf0pvGYb024826UBVQ6DLAgAAAUQIAtDjRVjM+s0lwzUsK15uQ1q551CgSwIAAAFECAIQFkwmk8b3SZIkrd5HCAIAIJwRggCEjXG9PSFo1V5CEAAA4YwQBCBsjO3lCUEbCyvV4HQd9bytxdW66rml+s/a/d1VGgAA6EaEIABhIzcpSulxdjldhtbvr2z3nP0V9brhxeVavrtccz/eIpc7ZHcRAAAAR0EIAhA2TCbTMafEHap16IYXlutAVaMkqbiqQd/sKO3WGgEAgP8RggCElaOFoHqHS7e8skI7D9YqKyFS5zVvqvqvVQXdXiMAAPCviEAXAADdaWxzCFq995AMw5DJZJIk3f+v77R6X4XiIyP0ys0T1OB06ZMNxfpsY7Eq651KiLIGsmwAAOBDjAQBCCvDs+NlizCrrNahvWV1kjyjQh+tK1KE2aQXbjxZgzLiNDInQYMyYtXY5NZH64oCXDUAAPAlQhCAsGKPsOiknARJh6fEPTF/myTpinG5OrlPsiTP+qErxuVKkv61Kj8AlQIAAH8hBAEIOy1T4lbtO6SVe8r11fZSRZhNunvagFbnXTomRxazSav3VWjnwZpAlAoAAPyAEAQg7LTsF7R67yE9MX+7JM8oUF5ydKvz0uMiNXVQmiQaJAAA0JMQggCEnbG9EyVJW4qr9fWO9keBWrRMiXt3dQF7BgEA0EMQggCEnfS4SPU6YtTnh+PbjgK1OHNouhKjrTpQ1ahFW0u6q0QAAOBHhCAAYallv6AIs0l3TW1/FEjyNFK4cnyeJOmlb/Z0R2kAAMDPCEEAwtL0YRmSpBsm9TnqKFCLGyb1ltkkfb2jVNsOVHdHeQAAwI8IQQDC0nkjMrX4/5uqX1ww9Ljn5iZF65zhmZKkl77Z7Zd6NhZWalNhlV+uDQAAWiMEAQhLJpNJvVNiZDabOnT+zaf1lSS9u3q/ymsdPq2lrKZRlz+zRFc+t1S1jU0+vTYAAGiLEAQAHTC+d5JG5iSoscmtt77d5z1uGIYam1wndO1PNxarwelWTWOT1u+vPNFSAQDAcRCCAKADTCaTbjq1jyTp1aV75Ghy67ONxbrgr19r5MOf67nFO+XuYgvtj9YVeb9fm1/hg2oBAMCxEIIAoIMuOClLaXF2Hahq1NQ/LtSPXlulTUVVcjS5NfeTLbr+xeU6UNXQqWuW1jRq2a4y789r91X4uGoAAPB9EYEuAABChT3Cousm9taf529TYWWDYmwW3XhqH2XGR+r3H2/RNzvKdO4TX+rKk/MUZbXIFmFWQpRV5w7PVEqsvd1rfrqhWG5DirFZVOtwaU3+oW5+VwAAhB9CEAB0wk2n9dHeslplJ0bp5tP6KjnGJkma1D9V97y9RhsLq/Tc4l2tnvOb/27SD8bk6KZT+2pwZlyrx1qmwt16ej89uXCHDlQ1qqiyXlkJUd3zhgAACEMmwzC6Nok9CFRVVSkhIUGVlZWKj48PdDkAwlxjk0tvLd+nveV1cjS55Whya3NxlTbsP9z6+uJR2frTlaMUYTHrYHWjJv5+vtyG9NVPpun211Zpc1GVnpk5VueNzPI+54vNB1Re69APmzdtBQAAbXUmGzASBAA+Yo+w6MZT+7Y6ZhiGVu09pBe/2a1PNxTrg+8KFW2zaO5lI/XpRs9UuFF5icpLjtbovERtLqrS2vwKbwgqr3XojtdXyeky1C8tVuN6JwXirQEA0KPQGAEA/MhkMml8n2Q9PXOcnrt+vMwm6e0V+Xpq4Q59tK5QknTBSM9GrGPyEiVJa47oEPff7wrldHkG7F9buqc7SwcAoMciBAFAN5k+LEMPXzxckvT/Pt+m5bvLJUnnN4/6jO6VKElaX1CpJpdbkvTv1QXe53+8vlilNY3dWDEAAD0TIQgAutENk/roR2f0kyQZhjQ6L1G5SdGSpP5psYq1R6je6dK2AzXafqBa6woqFWE2aVBGrBwut95ZkR/I8gEA6BEIQQDQzR44d4guHpUtSbrq5MPNDixmk0blJUjybJr67pr9kqSpg9N1x5T+kqQ3lu31jhIBAICuIQQBQDczm036y9WjteC+Kbr65NYd30Y3rwtatfeQ3m8OQZePzdH5I7OUHGNTYWWDvthS0unXNAxD/1m7X9sPVJ9w/QAAhDpCEAAEgMlkUr+0WJlMplbHR+d5ur/9d12hiiobFB8ZoTOHpivSatGVzS2yX1u6t9Ov9+mGYt3z9lrd9cbqEy8eAIAQRwgCgCDSMhLkaPJMebtoVLbsERZJ0syJvWQySV/vKNW2To7ovL7cE5y2l9RoX1md7woGACAEEYIAIIikxdmVkxjl/fmysbne7/OSo3XWkHRJ0ow/f6mRD32mMx9fpP99Z60q65xHveae0lp9s6PM+/PibZ2fTgcAQE9CCAKAINPSKrtPSrTGNn/f4u5pA5QUbZUkVTc2adfBWr23Zr9mvrBMFXWOdq/3dnNHOYvZM/Vu0daD/ikcAIAQQQgCgCDT0jnujin926wZGtMrSat/OV3rHp6hL+6bohdmjVdKjE0b9ldp5vPL2wQhR5Nb/1rlCUGzpw2QJC3ZWabGJlc3vBMAAIITIQgAgsw5wzO17bfn6eoJvdp93GQyKT7Sqv5psTpraIbeuv0UpcbatLGwStf+Y7kO1R4OQvM2HVBpjUPpcXbNPnOA0uPsqne6tGL3oe56OwAABB1CEAAEIVtEx389D8qI01u3naLUWLs2FVXpkqe+0Xf5FZKkt77dJ0m6cnyerBazpgxKkyQt2tqxdUGNTS4drG5UVYNThmF07k0AABCkIgJdAADgxA3MiNPbt0/UrBdXaF95na54doluOa2fvt5RKpPp8KasUwan6f9WFWjRtoP6RTvXcbsNPfflLr26dI/Kax1qbDq8MWu0zaLM+EhlJ0ZpYt9kTR2cruHZ8TKbTe1cCQCA4GUyQvif9qqqqpSQkKDKykrFx8cHuhwACLjKOqce+Pc6fbqx2HtsyqA0vXLzBO/jYx75XG5D+vqBacpNij783Hqn7vvnWs3f3PHucamxdk3sl6xhWfEanBGnodnxrbrbAQDQXTqTDRgJAoAeJCHaqmeuG6vXl+3VIx9tlqPJretP6d3q8TG9krRq7yEt3nZQMyd6HttYWKk7X1+tfeV1skWY9dBFwzRlUJriIq2KtUfI0eRWcVWDiirrtetgrb7cdlDf7ChVaU2jPlpXpI/WFXlf4xcXDNWtp/fr9vcOAEBHMRIEAD3U7tJa5ZfX6YzmdUAt/vbFdj0+b5umD8vQX68eo2cW7dCzX+6So8mt3KQoPXvdOI3ISTju9R1Nbq3cU67vCiq1tbhKGwqrtKOkRqmxNn3z0zO9m7wCANAdOpMNCEEAEGbWF1Tqoie/VpTVopRYmwoO1UuSzhqSrsevHKXEaFuXrut0uXX6YwtVXNWgx384SpePyz3+kwAA8JHOZAO6wwFAmBmeHa/UWJvqnS4VHKpXTmKUnr1urJ6fNb7LAUiSrBazrp/kmV730pLddJMDAAQtQhAAhBmz2aQbJ/dRfGSE7p7WX/PuPUPnjshqszFrV1wzoZfsEWZt2F+lVXvZiwgAEJxojAAAYWj2mQM1+8yBPr9ucoxNl47O0Tsr8/XSN3s0vk+yz18DAIATxUgQAMCnbjqtjyTp043FKqyoD2wxAAC0gxAEAPCpIZnxmtQvRS63oVeX7g10OQAAtMF0OACAz910ah8t3VWmF7/erXmbihUXaVVcZITiI62Kj4pQXKRVOYlROm9EptLjIwNdLgAgzBCCAAA+d9bQDA3JjNOW4mrtPFh71PN+/d+NOn1gmi4fl6tzhmewtxAAoFuwTxAAwC8anC7tKKlRdUOTqhucqm5oUlXz/69ucGr1vopWHeTG9ErUqzdPUFyktUuvV+9waemuUtksFsVGRijWHqFeydGyRTDzGwDCQWeyASNBAAC/iLRaNCIn4Zjn7C6t1XurC/TK0r1as69CN7+8Qi/fNEEx9s79z5NhGJrzzhp9tvFAq+MD0mP12ZwzZDGfePtvAEDPwT+PAQACpm9qjO6dMVhv3DpR8ZERWrHnkG59ZaXqHS6VVDfo+a926bKnv9Ev39+gqgbnUa/zn7WF+mzjAVktJg3JjFNOYpQsZpN2lNSwXxEAoA2mwwEAgsLa/Apd9/xy1TQ2KScxSkWV9XIf8b9QWQmR+v1lIzVtcHqr55VUNWj6n79UZb1T900fpB+f5dn/6H/fWav31uzXraf11S8uHNadbwUAEACdyQaMBAEAgsLovES9fNPJirZZtL/CE4DG9ErUT84drN4p0SqqbNBNL63Qff/8TvvK6iR5psH97L31qqx3akROvO6Y2t97vXOGZ0iSPt90QCH8730AAD9gJAgAEFQ27K/UNztKNX1YhvqlxUryND3442db9dKS3TIMyWSSzhycrkGZcXpm0U5ZLSb998enaUjm4f8tqHM0acxv5qmxya1P55ze6jEAQM/DSBAAIGSNyEnQj6b09wYgSYqyWfSri4bpX3dM0hmD0mQY0hdbSvTMop2SpDlnD2oTcqJtETp9YJok6bMNrRsmAADCGyEIABAyxvVO1qs3T9CC+6bo5lP7Kj4yQpP7p+hHZ/Rr9/wZ3ilxxd1ZJgAgyNEiGwAQcvqlxepXFw3Try4aJsMwZDK13wL7rCHpMpukjYVVKjhUp9yk6GNe1+lyq7SmUVaLWamxdn+UDgAIAoQgAEBIO1oAkqSUWLtO7pOs5bvL9fnGA7r5tL6tHjcMQx+tL9LzX+1WwaE6ldU61LJSdnh2vM4ckq5pQ9I1Ji/xmK8DAAgtTIcDAPRoM4ZnSmo7JW5HSbVmPr9cs99co7X5FSqt8QSgCLNJpubRo78t2KHLnl6iO15fJafLHYjyAQB+wEgQAKBHmzEsQ498uEnf7i7XpsIqbS+p1tfbS/Xemv1qchuyR5h159T+mjEsU+nxdiVH23SozqFFWw9qwdYSzdt0QJ9tPKA5b6/VX64erQhL5//98LnFO/WXL7brlZsn6OQ+yX54lwCAzqBFNgCgxzv/L19pU1FVm+PTh2XoVxcOU17y0dcKLdpaottfXSWHy61LR2fr8StHy2Lu+NS4Jpdbp8z9QqU1Dp02IFWv3zqxS+8BAHBsnckGjAQBAHq8y8flatOHm2Q2eVpwT+iTrDOHpmty/9TjPnfq4HQ9ee0Y3fXGar2/tlAOl1uT+qcqxmZRtC1CaXF25SZFKS3WLnM74WjJzjKV1jgkSV/vKNXW4moNzozr0vv40+dbdbDGoUcuGd6lESkAgAchCADQ4900uY9OG5Cq7MRIxUVaO/38GcMz9cTVo/U/b63Rx+uL9fH6ti23bRazBqTH6h+zxisnMcp7/D9rCyVJZpPkNqQXv96tx644qdM1rNxTrr8u2CFJmtA3ST8Yk9vpawAAPPhnJABAj2c2mzQ4M65LAajFhSdl6+WbJuiysTk6d3imTh+YqjG9EpWTGCWzSXK43NpUVKXHP9/qfU6D06XPNnoC00/OHSJJem/tfpXVNHb69f/yxXbv939bsEMud8jOZgeAgGMkCACADjpjUJrOGJTW5niTy61vd5fr2ueX6/01+3XX1P4akB6nLzaXqKaxSTmJUbr99H76eH2R1hVU6s3l+/TjswZ2+HVX7S3XV9tLFWE2Kcpm0a6DtfpwXaEuGZ3jy7cHAGGDkSAAAE5QhMWsyQNSNWNYhtyG9Of5nlGb/6zdL0m6ZHS2zGaTbmnep+jVZXvV2OTq8PWfaL7e5WNzdfvp/SR5RoPcjAYBQJcQggAA8JF7ZwySySR9tK5IS3eWadHWg5LkHbE5b0SWMuLtOljdqI/WFXXomqv3HfKOAt09bYBmndpH8ZER2lFSo483dOwaXfXKkj069dEFWptf4dfX6U6GYWhfWZ1CuDkuAB8gBAEA4CNDMuN14UnZkqQ7Xve01R6SGeftBmeLMOuGSX0kSY9/vk2fbig+7mjOX5pHgS4bm6NeKdGKj7Tq5uYRpb994RkNamxyacP+Sm07UN3pmhucLq0rqGgTCr7ZUapf/3ej9lfU6/99tvUozw49f/liu87440K9+M2eQJcCIIBYEwQAgA/NOXugPlpXqMp6pyTp4tHZrR6/dkIvvbZ0r/ZX1OuO11dpWFa87prWX9mJUfLkEEOHap3aV16nnQdrtHjbQVnMJs2edngN0U2T++qFr3Zr64Fqnfn4IhUcqleT25DJJD133TjNGJ7ZoVobm1y68rmlWldQqR+MydHcy0Yq0mpRcWWD/uetNWrJZ1/vKNWG/ZUakZPQ6c/DMAw99+UuFVXU62cXDJU9wtLpa/jKzoM1emqhp8Pey0t266bJfdptaw6g5yMEAQDgQ/3TYnX52Fz936oCSdLFo1qHoKQYmz6dc7qe/2q3XvpmtzYVVWn2m2uOec0rxuaqV8rhDV0Toq266dQ++uuCHdpTVidJirSa1eB0a847a/WvOyZrWPbhjQI3FlZqa3G1Lh6V3Wp/obkfb9G6gkpJ0ntr9mtvWa2enjlOs99crbJah4ZkxqlPSow+3Vis577cpb9dM6bTn8ffv9ylRz/ZIklqchv63Q9GdvoavmAYhh7+YKOcLk+yyy+v14o95ZrYLyUg9QAILJMRwpNiO7MrLAAA3aXgUJ0ue3qJJvZLOWZwOFTr0D++2qVPNhTL6XLLZJJMMik+KkK9kqOVlxytvikxumhUtmLsrf/d0tHk1vtr9ys52qah2fFKj7PrppdW6OsdpcpOiNT7s09VYpRNf/1iu55etENuQzp1QIqevGaskmJs+mR9ke58Y7Uk6Z6zBuqlb3arqqFJ9gizGpvcirNH6L8/Pk21jiZd8NevZTGbtOj+qcpLjm7vrbRr/qYDuu21lTryL43HfzhKl4/r/j2OPl5fpLveWC1bhFkT+ybrq+2l+uG4XP3xh6O6vRYA/tGZbEAIAgDADwzDkMnUvVOtKuuc+sHT32hXaa1G5SbI6TK0qahKkmS1mOR0GeqVHK1fXThM//vOWlU3NulHU/rpwfOGatfBGt3yykrtLq2VJD173TidO8Izre76F5brq+2lunFyHz188fAO1bK5qEpXPLNEtQ6XZk7spbQ4u56Yv12RVrPeu+tUDc3qvv/drnM06azHF6uoskH/c9ZAnT4wVT98dqmibRat+PnZbQImgNDUmWzAf+sBAPCD7g5Akmea3POzxuvSp77Rd83T3JKirfr9D0aqb1qMbnt1pfaV1+nWV1dKksb1TtL9MwZLkvqlxeq9uybrT/O2aVhWvDcASdKPzuivr7aX6p0V+brnrIFKirGpwenSluJqHapzqLqhSVX1TtU7XHK43GpscuvfqwpU63Bpcv8UPXzxcFlMJq3Nr9CirQd1x+ur9PTMsYq1RyjKalGkzaLICIusFpNMJpMcTW6V1TaqpKpRtgjzCQemvy3YoaLKBuUmRemuqf1ljzCrd0q09pbV6dMNxQEZmQIQWIwEAQDQwyzZUaofv7VGJ/dJ1iOXjlBanF2SVF7r0N1vrNbSXWVKjLbq4/85XdmJUce9nmEYuvBvX2tjYZXOGpIuR/PmsI1N7mM+r09KtN6/+1QlRtskeab/Xfi3r7W/or7d8y1mk+wRZtU5Wu+hNG1wmn554TD1S4vtyNv3OljdqEc/2aJ/r/asz/rHDeM1fViGJOlvX2zX4/O2aVK/FL11+ymdui6A4MR0OAAAwtzRpuM5XW59uK5QJ+Umqn8nQsUH3xXqf95q3cAhNdamjPhIxUdaFR8VoWhbhGwWs2wRZsVHRei6U3orK6F1yNpYWKkH312v/YfqVe90qcHpUntdwiPMJqXF2VVa0yiny5DVYtLNp/XVj88cqNjjTF9zutx6Y9lePT5vm6obmiRJt5zWV7+4YKj3M9lfUa/THlsgw5C++sm0Tq11kjytxUtrGpUZH9mq2QSAwCEEAQAAn2pyufXgu+tVVuvQqQNSdfrAVA1Mjz3haX+GYcjhcqvB6VaD06VGp1txkRFKiLLKbDZp18EaPfLhJi1s3ng2Jcam2WcO0LUTe7Vpt72vrE5vr9in/1tVoIPVjZKkETnx+s0lIzS2V1Kb1575/DJ9s6NM/3v2IN1z9sA2j7enss6pV5fu0UtL9qi81iGbxaw+qdHqnxaryf1TdOFJ2UqKsZ3QZwKgawhBAACgR1mw5YB+++Fm7Wpu3JCbFKWbTu2rekeT9pTVaUdJjdbmV3jPT421a87ZA3XNhF6yHGUvoPfX7Necd9YqLjJCQzPjZY0wKcJsliHJ7TbkNgxZzCYlRFmVEGWV2zD0wdpC1TZP1zOb1GYUy2oxadrgdP1gTI5OH5R23FErAL5DCAIAAD2O0+XWP1fm6y/zt6ukeaTnSCaTdPrANF1zcp7OGpohW8Sxp6nVO1w69bEFKq91dKqOIZlxunNqf50/MksHqhq082CtNhVW6cN1hdpYWOU9z2oxaXzvZE0ZnCbDkLYUV2lLUbUOVDeoX2qMhmXHa2hWvJKjbXK6DTmb3DKbpYl9U9pdqxWIjoNAKCEEAQCAHqve4dLLS/Zoyc5SZcRHqndytHqnxmhsr0TlJnVubc/+inpt3F+pJrchp8stR5NbZpNJZrNkNnnailfWO1VV71Sdo0mT+6dq6uC0o4aRrcXVendNgT7bUOzdyLYrRuUm6JwRmUqIsmrNvgqt2XdIu0trNTw7QdOGpGva4DRlJURpS3GVthZXa9fBWkVazUqKsSkp2qbMhEiNyUtUenykJM/I1ncFFfp0Y7EKKxo0KjdB4/ska3h2vKwdWNNU1eBUQXm9BmbEduh8IBAIQQAAAAG2p7RWi7aW6JudZbI3t/oemhWnjPhI7Sip0aaiKm0uqla9o0lWi1lWi1mV9U59V1AhX/11lpMYpWHZ8VpfUKniqoY2j0dZLeqbGqOshEhlJUYqIy5SCdFWxUdaFWuP0OaiKn25/aBW76uQy20ozh6h0welatrgdCXH2LS7tFa7Smt1oLJBWYmR6pcaq75pMYq2WlRa41BZbaOq6p3qlxarMb0S2zTKAHwppELQ008/rT/+8Y8qKirS8OHD9cQTT+j000/v0HMJQQAAoKcpqW7QvE0HNH/TATldhkbnJWpMr0T1TY3Rqr2HtHBrib7aVqpaR5P6pcVqcGacBqbHqsllqLzOoUO1Du0urdXWA9WtwlSsPULThqRrcEas1uZXaOXeQ6qoc3a4riirRfVO1/FPPIaMeLt6J8fI6XbL6XKryWUoLc6uvORo5SVFKzXWJkOeqX9Nbs8oXHmNQ+W1DtU7XZ71WdFWJUbZlBhtVWLzz3F2q8xmySSTTCbpQFWDdh2s1a7SGpVUNSonKUr9UmPUNzVWSTFWGYZkGFKT263KeqcO1TlUXuuULcKsgemxGpQRp+QYm2obm7S7tFY7D9aozuFSZnykMuIjlRFvV2ykpxvi8aYoGoahqoYmVdQ5VFHnlNlkUpTNohi7RdHWCEXbLYyu+UjIhKB33nlH119/vZ5++mmdeuqpeu655/T8889r06ZN6tWr13GfTwgCAADhqMnllssw2nTIO1J1g1PrCiq1qbBK/dJidOqAVEVaD5/vdhvaVVqr/PI6FVbWq7iyQSVVjapqcHq+6puUnRipMwal6YyBacpJjNJ3BRVauKVEi7eXytnkVt+0GPVPjVF6fKQKK+o9I0MHa+VwuZUaa1NqrF3RtgjPeqjiarna64cepOLsEapubDruefYIsyKtFtkjzLJbzbJHWOR0uVXvcKne6VKdw3Xc9221mBRti1BKrE15SdHqlRytjHi7ah0uVdY7VVnnlCFDsfYIxdqtskWYVVLVoIJD9co/VCeny630OE84S4uzy2I2SzK8Yc+QIfcR3zf/n5rchhqaW9U3OF2KMJsVbfNsYBxri1BijFXJ0Z4plpE2i8wmzzTRJreh0upGHaxp1MHqRrnchv581WiffO4nImRC0MSJEzV27Fg988wz3mNDhw7VpZdeqrlz5x73+YQgAACA0FDnaNKG/VUqqW6QzWKWNcIss8mkA5UNyj9Up/zyOh2qc8ps8mycazKZFB9pVWqsTckxNkXZLKqqd6qizqlDdU5V1ntGVirqnapucDb/ge8ZeUmOsalfaqz6pcUoPc6u/S0BrbRWNQ1NMpk8o0Yt3f+SYzwjS/UOl7aVVCu//PCGvikxNvVLi1GsPUIHqhp1oKpBZZ1spiFJ0TaLEqOskqRah0t1jiY5XaETCo8lwmzStt+eJ/NROjF2l85kg4D1bXQ4HFq1apV++tOftjo+Y8YMLVmypN3nNDY2qrHxcDeYqqqqds8DAABAcIm2RWhC3+RAl9EhdY4m7T9Ur/TmNVLf53Q172vVdPj/NzrdamxyqcHpli3CpEirRVFWi6JtEUqMtrYahWvhaPKMGNU5m1Tb2KQDVY3KL69T/qE6lVQ1KrZ5z6yEKKvMJpNqGptU09ikBqdL6XGRyk2KUm5SlGdkqLpRJVUNzSMznm6JZpO80/VMzaM4pubvW0JgpNWiSKtnNKvJbaje0aR6h0s1jU06VOfUoVqHymodcjS55TY8o0smk5QaZ1darGfkKT3OLrdhyKzQ6V4YsBBUWloql8uljIyMVsczMjJUXFzc7nPmzp2rX//6191RHgAAAMJUtC1CAzPijvp4SyOLo5/RMbYIs2wRZiXIE7QGpHf9isNPsJZwE/BVWN9fTHasHvgPPvigKisrvV/5+fndUSIAAACAHiRgI0GpqamyWCxtRn1KSkrajA61sNvtstvt3VEeAAAAgB4qYCNBNptN48aN07x581odnzdvniZPnhygqgAAAAD0dAEbCZKke++9V9dff73Gjx+vSZMm6e9//7v27dunO+64I5BlAQAAAOjBAhqCrrrqKpWVlek3v/mNioqKNGLECH388cfq3bt3IMsCAAAA0IMFdJ+gE8U+QQAAAACkzmWDgHeHAwAAAIDuRAgCAAAAEFYIQQAAAADCCiEIAAAAQFghBAEAAAAIK4QgAAAAAGGFEAQAAAAgrBCCAAAAAIQVQhAAAACAsEIIAgAAABBWCEEAAAAAwgohCAAAAEBYIQQBAAAACCuEIAAAAABhhRAEAAAAIKwQggAAAACEFUIQAAAAgLBCCAIAAAAQVghBAAAAAMJKRKALOBGGYUiSqqqqAlwJAAAAgEBqyQQtGeFYQjoEVVdXS5Ly8vICXAkAAACAYFBdXa2EhIRjnmMyOhKVgpTb7VZhYaHi4uJkMpkCWktVVZXy8vKUn5+v+Pj4gNbSU/EZ+xefr//xGfsXn6//8Rn7F5+v//EZ+1egP1/DMFRdXa3s7GyZzcde9RPSI0Fms1m5ubmBLqOV+Ph4/kvlZ3zG/sXn6398xv7F5+t/fMb+xefrf3zG/hXIz/d4I0AtaIwAAAAAIKwQggAAAACEFUKQj9jtdj300EOy2+2BLqXH4jP2Lz5f/+Mz9i8+X//jM/YvPl//4zP2r1D6fEO6MQIAAAAAdBYjQQAAAADCCiEIAAAAQFghBAEAAAAIK4QgAAAAAGGFEOQjTz/9tPr27avIyEiNGzdOX331VaBLCklz587VySefrLi4OKWnp+vSSy/V1q1bW51z4403ymQytfo65ZRTAlRx6Hn44YfbfH6ZmZnexw3D0MMPP6zs7GxFRUVp6tSp2rhxYwArDi19+vRp8/maTCbdfffdkrh/u+LLL7/URRddpOzsbJlMJr3//vutHu/IPdvY2Kgf//jHSk1NVUxMjC6++GIVFBR047sIXsf6fJ1Opx544AGNHDlSMTExys7O1g033KDCwsJW15g6dWqb+/rqq6/u5ncSvI53D3fk9wL38NEd7/Nt73eyyWTSH//4R+853MNH15G/zULx9zAhyAfeeecdzZkzRz//+c+1Zs0anX766TrvvPO0b9++QJcWchYvXqy7775by5Yt07x589TU1KQZM2aotra21XnnnnuuioqKvF8ff/xxgCoOTcOHD2/1+a1fv9772B/+8Af96U9/0pNPPqkVK1YoMzNT06dPV3V1dQArDh0rVqxo9dnOmzdPkvTDH/7Qew73b+fU1tZq1KhRevLJJ9t9vCP37Jw5c/Tee+/p7bff1tdff62amhpdeOGFcrlc3fU2gtaxPt+6ujqtXr1av/zlL7V69Wq9++672rZtmy6++OI25952222t7uvnnnuuO8oPCce7h6Xj/17gHj66432+R36uRUVFevHFF2UymXT55Ze3Oo97uH0d+dssJH8PGzhhEyZMMO64445Wx4YMGWL89Kc/DVBFPUdJSYkhyVi8eLH32KxZs4xLLrkkcEWFuIceesgYNWpUu4+53W4jMzPTePTRR73HGhoajISEBOPZZ5/tpgp7lnvuucfo37+/4Xa7DcPg/j1Rkoz33nvP+3NH7tmKigrDarUab7/9tvec/fv3G2az2fj000+7rfZQ8P3Ptz3ffvutIcnYu3ev99iUKVOMe+65x7/F9RDtfcbH+73APdxxHbmHL7nkEuPMM89sdYx7uOO+/7dZqP4eZiToBDkcDq1atUozZsxodXzGjBlasmRJgKrqOSorKyVJycnJrY4vWrRI6enpGjRokG677TaVlJQEoryQtX37dmVnZ6tv3766+uqrtWvXLknS7t27VVxc3Op+ttvtmjJlCvdzFzgcDr3++uu6+eabZTKZvMe5f32nI/fsqlWr5HQ6W52TnZ2tESNGcF93QWVlpUwmkxITE1sdf+ONN5Samqrhw4fr/vvvZ/S4k471e4F72HcOHDigjz76SLfcckubx7iHO+b7f5uF6u/hiIC8ag9SWloql8uljIyMVsczMjJUXFwcoKp6BsMwdO+99+q0007TiBEjvMfPO+88/fCHP1Tv3r21e/du/fKXv9SZZ56pVatWhcQOxYE2ceJEvfrqqxo0aJAOHDig3/72t5o8ebI2btzovWfbu5/37t0biHJD2vvvv6+KigrdeOON3mPcv77VkXu2uLhYNptNSUlJbc7h93TnNDQ06Kc//amuvfZaxcfHe4/PnDlTffv2VWZmpjZs2KAHH3xQ3333nXc6KI7teL8XuId955VXXlFcXJwuu+yyVse5hzumvb/NQvX3MCHIR478V17Jc5N8/xg6Z/bs2Vq3bp2+/vrrVsevuuoq7/cjRozQ+PHj1bt3b3300UdtfqmhrfPOO8/7/ciRIzVp0iT1799fr7zyinchLvezb7zwwgs677zzlJ2d7T3G/esfXblnua87x+l06uqrr5bb7dbTTz/d6rHbbrvN+/2IESM0cOBAjR8/XqtXr9bYsWO7u9SQ09XfC9zDnffiiy9q5syZioyMbHWce7hjjva3mRR6v4eZDneCUlNTZbFY2qTYkpKSNokYHffjH/9YH3zwgRYuXKjc3NxjnpuVlaXevXtr+/bt3VRdzxITE6ORI0dq+/bt3i5x3M8nbu/evZo/f75uvfXWY57H/XtiOnLPZmZmyuFw6NChQ0c9B8fmdDp15ZVXavfu3Zo3b16rUaD2jB07Vlarlfu6i77/e4F72De++uorbd269bi/lyXu4fYc7W+zUP09TAg6QTabTePGjWszXDpv3jxNnjw5QFWFLsMwNHv2bL377rtasGCB+vbte9znlJWVKT8/X1lZWd1QYc/T2NiozZs3KysryzsV4Mj72eFwaPHixdzPnfTSSy8pPT1dF1xwwTHP4/49MR25Z8eNGyer1drqnKKiIm3YsIH7ugNaAtD27ds1f/58paSkHPc5GzdulNPp5L7uou//XuAe9o0XXnhB48aN06hRo457LvfwYcf72yxkfw8HpB1DD/P2228bVqvVeOGFF4xNmzYZc+bMMWJiYow9e/YEurSQc+eddxoJCQnGokWLjKKiIu9XXV2dYRiGUV1dbdx3333GkiVLjN27dxsLFy40Jk2aZOTk5BhVVVUBrj403HfffcaiRYuMXbt2GcuWLTMuvPBCIy4uznu/Pvroo0ZCQoLx7rvvGuvXrzeuueYaIysri8+3E1wul9GrVy/jgQceaHWc+7drqqurjTVr1hhr1qwxJBl/+tOfjDVr1ni7k3Xknr3jjjuM3NxcY/78+cbq1auNM8880xg1apTR1NQUqLcVNI71+TqdTuPiiy82cnNzjbVr17b6vdzY2GgYhmHs2LHD+PWvf22sWLHC2L17t/HRRx8ZQ4YMMcaMGcPn2+xYn3FHfy9wDx/d8X5HGIZhVFZWGtHR0cYzzzzT5vncw8d2vL/NDCM0fw8TgnzkqaeeMnr37m3YbDZj7NixrVo6o+Mktfv10ksvGYZhGHV1dcaMGTOMtLQ0w2q1Gr169TJmzZpl7Nu3L7CFh5CrrrrKyMrKMqxWq5GdnW1cdtllxsaNG72Pu91u46GHHjIyMzMNu91unHHGGcb69esDWHHo+eyzzwxJxtatW1sd5/7tmoULF7b7e2HWrFmGYXTsnq2vrzdmz55tJCcnG1FRUcaFF17I597sWJ/v7t27j/p7eeHChYZhGMa+ffuMM844w0hOTjZsNpvRv39/43/+53+MsrKywL6xIHKsz7ijvxe4h4/ueL8jDMMwnnvuOSMqKsqoqKho83zu4WM73t9mhhGav4dNhmEYfhpkAgAAAICgw5ogAAAAAGGFEAQAAAAgrBCCAAAAAIQVQhAAAACAsEIIAgAAABBWCEEAAAAAwgohCAAAAEBYIQQBAAAACCuEIABA2OjTp4+eeOKJQJcBAAgwQhAAwC9uvPFGXXrppZKkqVOnas6cOd322i+//LISExPbHF+xYoVuv/32bqsDABCcIgJdAAAAHeVwOGSz2br8/LS0NB9WAwAIVYwEAQD86sYbb9TixYv1l7/8RSaTSSaTSXv27JEkbdq0Seeff75iY2OVkZGh66+/XqWlpd7nTp06VbNnz9a9996r1NRUTZ8+XZL0pz/9SSNHjlRMTIzy8vJ01113qaamRpK0aNEi3XTTTaqsrPS+3sMPPyyp7XS4ffv26ZJLLlFsbKzi4+N15ZVX6sCBA97HH374YY0ePVqvvfaa+vTpo4SEBF199dWqrq7274cGAPArQhAAwK/+8pe/aNKkSbrttttUVFSkoqIi5eXlqaioSFOmTNHo0aO1cuVKffrppzpw4ICuvPLKVs9/5ZVXFBERoW+++UbPPfecJMlsNuuvf/2rNmzYoFdeeUULFizQT37yE0nS5MmT9cQTTyg+Pt77evfff3+bugzD0KWXXqry8nItXrxY8+bN086dO3XVVVe1Om/nzp16//339eGHH+rDDz/U4sWL9eijj/rp0wIAdAemwwEA/CohIUE2m03R0dHKzMz0Hn/mmWc0duxY/f73v/cee/HFF5WXl6dt27Zp0KBBkqQBAwboD3/4Q6trHrm+qG/fvnrkkUd055136umnn5bNZlNCQoJMJlOr1/u++fPna926ddq9e7fy8vIkSa+99pqGDx+uFStW6OSTT5Ykud1uvfzyy4qLi5MkXX/99friiy/0u9/97sQ+GABAwDASBAAIiFWrVmnhwoWKjY31fg0ZMkSSZ/Slxfjx49s8d+HChZo+fbpycnIUFxenG264QWVlZaqtre3w62/evFl5eXneACRJw4YNU2JiojZv3uw91qdPH28AkqSsrCyVlJR06r0CAIILI0EAgIBwu9266KKL9Nhjj7V5LCsry/t9TExMq8f27t2r888/X3fccYceeeQRJScn6+uvv9Ytt9wip9PZ4dc3DEMmk+m4x61Wa6vHTSaT3G53h18HABB8CEEAAL+z2WxyuVytjo0dO1b//ve/1adPH0VEdPx/jlauXKmmpiY9/vjjMps9Exr++c9/Hvf1vm/YsGHat2+f8vPzvaNBmzZtUmVlpYYOHdrhegAAoYfpcAAAv+vTp4+WL1+uPXv2qLS0VG63W3fffbfKy8t1zTXX6Ntvv9WuXbv0+eef6+abbz5mgOnfv7+ampr0t7/9Tbt27dJrr72mZ599ts3r1dTU6IsvvlBpaanq6uraXOfss8/WSSedpJkzZ2r16tX69ttvdcMNN2jKlCntTsEDAPQchCAAgN/df//9slgsGjZsmNLS0rRv3z5lZ2frm2++kcvl0jnnnKMRI0bonnvuUUJCgneEpz2jR4/Wn/70Jz322GMaMWKE3njjDc2dO7fVOZMnT9Ydd9yhq666SmlpaW0aK0ieaW3vv/++kpKSdMYZZ+jss89Wv3799M477/j8/QMAgovJMAwj0EUAAAAAQHdhJAgAAABAWCEEAQAAAAgrhCAAAAAAYYUQBAAAACCsEIIAAAAAhBVCEAAAAICwQggCAAAAEFYIQQAAAADCCiEIAAAAQFghBAEAAAAIK4QgAAAAAGHl/wfuec2fIfw5KAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "\n",
    "data = load_coco_data(max_train=50)\n",
    "\n",
    "transformer = CaptioningTransformer(\n",
    "          word_to_idx=data['word_to_idx'],\n",
    "          input_dim=data['train_features'].shape[1],\n",
    "          wordvec_dim=256,\n",
    "          num_heads=2,\n",
    "          num_layers=2,\n",
    "          max_length=30\n",
    "        )\n",
    "\n",
    "\n",
    "transformer_solver = CaptioningSolverTransformer(transformer, data, idx_to_word=data['idx_to_word'],\n",
    "           num_epochs=100,\n",
    "           batch_size=25,\n",
    "           learning_rate=0.001,\n",
    "           verbose=True, print_every=10,\n",
    "         )\n",
    "\n",
    "transformer_solver.train()\n",
    "\n",
    "# Plot the training losses.\n",
    "plt.plot(transformer_solver.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcRNyolJ7qyw"
   },
   "source": [
    "Print final training loss. You should see a final loss of less than 0.05 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JPU95Nv27qyx",
    "tags": [],
    "test": "transformer_final_training_loss"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss:  0.013547476\n"
     ]
    }
   ],
   "source": [
    "print('Final loss: ', transformer_solver.loss_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R-SUFxf7qyx"
   },
   "source": [
    "# Transformer Sampling at Test Time\n",
    "The sampling code has been written for you. You can simply run the following to compare with the previous results with the RNN. As before the training results should be much better than the validation set results, given how little data we trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "K4uQMkIC7qyy",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://farm1.staticflickr.com/202/487987371_489a65d670_z.jpg\n",
      "train\n",
      "a broken cross walk signal leaning to the side <END>\n",
      "GT:<START> a broken cross walk signal leaning to the side <END>\n",
      "http://farm6.staticflickr.com/5292/5448656470_53629afa03_z.jpg\n",
      "train\n",
      "a <UNK> decorated living room with a big tv in it <END>\n",
      "GT:<START> a <UNK> decorated living room with a big tv in it <END>\n",
      "http://farm1.staticflickr.com/25/44101107_9491d72776_z.jpg\n",
      "val\n",
      "a kitchen with a striped <UNK> and a sidewalk with drinking way of <END>\n",
      "GT:<START> a bedroom with a bed desk and <UNK> <UNK> <END>\n",
      "http://farm8.staticflickr.com/7096/7203872212_2782838eb5_z.jpg\n",
      "val\n",
      "a man dog with a stuffed office his food <UNK> <END>\n",
      "GT:<START> a group of people <UNK> outside by a wall <END>\n"
     ]
    }
   ],
   "source": [
    "# If you get an error, the URL just no longer exists, so don't worry!\n",
    "# You can re-sample as many times as you want.\n",
    "for split in ['train', 'val']:\n",
    "    minibatch = sample_coco_minibatch(data, split=split, batch_size=2)\n",
    "    gt_captions, features, urls = minibatch\n",
    "    gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
    "\n",
    "    sample_captions = transformer.sample(features, max_length=30)\n",
    "    sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
    "\n",
    "    for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
    "        # img = image_from_url(url)\n",
    "        # Skip missing URLs.\n",
    "        # if img is None: continue\n",
    "        # plt.imshow(img)\n",
    "        # plt.title('%s\\n%s\\nGT:%s' % (split, sample_caption, gt_caption))\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "        print(url)\n",
    "        print('%s\\n%s\\nGT:%s' % (split, sample_caption, gt_caption))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZMSyzMWyQpK"
   },
   "source": [
    "# Vision Transformer (ViT)\n",
    "\n",
    "[Dosovitskiy et. al.](https://arxiv.org/abs/2010.11929) showed that applying a transformer model on a sequence of image patches (referred to as Vision Transformer) not only achieves impressive performance but also scales more effectively than convolutional neural networks when trained on large datasets. We will build a version of Vision Transformer using our existing implementation of transformer components and train it on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAntTB4F0yHC"
   },
   "source": [
    "Vision Transformer converts input image into a sequence of patches of fixed size and embed each patch into a latent vector. In `cs231/transformer_layers.py`, complete the implementation of `PatchEmbedding` and test it below. You should see relative error less than 1e-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dRiRu3pN7qyz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  5.93853582728669e-06\n"
     ]
    }
   ],
   "source": [
    "from cs231n.transformer_layers import PatchEmbedding\n",
    "\n",
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "\n",
    "N = 2\n",
    "HW = 16\n",
    "PS = 8\n",
    "D = 8\n",
    "\n",
    "patch_embedding = PatchEmbedding(\n",
    "    img_size=HW,\n",
    "    patch_size=PS,\n",
    "    embed_dim=D\n",
    ")\n",
    "\n",
    "x = torch.randn(N, 3, HW, HW)\n",
    "output = patch_embedding(x)\n",
    "\n",
    "\n",
    "expected_output = np.asarray([\n",
    "        [[-0.6312704 ,  0.02531429,  0.6112642 , -0.49089882,\n",
    "          0.01412961, -0.6959372 , -0.32862484, -0.45402682],\n",
    "        [ 0.18816411, -0.08142513, -0.9829535 , -0.23975623,\n",
    "         -0.23109074,  0.97950286, -0.40997326,  0.7457837 ],\n",
    "        [ 0.01810865,  0.15780598, -0.91804236,  0.36185235,\n",
    "          0.8379501 ,  1.0191797 , -0.29667392,  0.20322265],\n",
    "        [-0.18697818, -0.45137224, -0.40339014, -1.4381214 ,\n",
    "         -0.43450755,  0.7651071 , -0.83683825, -0.16360264]],\n",
    "\n",
    "       [[-0.39786366,  0.16201034, -0.19008337, -1.0602452 ,\n",
    "         -0.28693503,  0.09791763,  0.26614824,  0.41781986],\n",
    "        [ 0.35146567, -0.4469593 , -0.1841726 ,  0.45757473,\n",
    "         -0.61304873, -0.29104248, -0.16124889, -0.14987172],\n",
    "        [-0.2996967 ,  0.27353522, -0.09929767,  0.01973832,\n",
    "         -1.2312065 , -0.6374332 , -0.22963578,  0.55696607],\n",
    "        [-0.93818814,  0.02465284, -0.21117875,  1.1860403 ,\n",
    "         -0.06137538, -0.21062079, -0.094347  ,  0.50032747]]])\n",
    "\n",
    "print('error: ', rel_error(expected_output, output.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-equGLTX7rxV"
   },
   "source": [
    "The sequence of patch vectors is processed by transformer encoder layers, each consisting of a self-attention and a feed-forward module. Since all vectors attend to one another, attention masking is not strictly necessary. However, we still implement it for the sake of consistency.\n",
    "\n",
    "Implement `TransformerEncoderLayer` in `cs231n/transformer_layers.py` and test it below. You should see relative error less than 1e-6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-YIr_Fxv5xvy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  1.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "\n",
    "from cs231n.transformer_layers import TransformerEncoderLayer\n",
    "\n",
    "N, T, TM, D = 1, 4, 5, 12\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(D, 2, 4*D)\n",
    "x = torch.randn(N, T, D)\n",
    "x_mask = torch.randn(T, T) < 0.5\n",
    "\n",
    "output = encoder_layer(x, x_mask)\n",
    "\n",
    "expected_output = np.asarray([\n",
    "    [[-0.43529928, -0.204897, 0.45693663, -1.1355408, 1.8000772,\n",
    "      0.24467856, 0.8525885, -0.53586316, -1.5606489, -1.207276,\n",
    "      1.3986266, 0.3266182],\n",
    "     [0.06928468, 1.1030475, -0.9902548, -0.34333378, -2.1073136,\n",
    "      1.1960536, 0.16573538, -1.1772276, 1.2644588, -0.27311313,\n",
    "      0.29650143, 0.7961618],\n",
    "     [0.28310525, 0.69066685, -1.2264299, 1.0175265, -2.0517688,\n",
    "     -0.10330413, -0.5355796, -0.2696466, 0.13948536, 2.0408154,\n",
    "      0.27095756, -0.25582793],\n",
    "     [-0.58568114, 0.8019579, -0.9128079, -1.6816932, 1.1572194,\n",
    "      0.39162305, 0.58195484, 0.7043353, -1.27042, -1.1870497,\n",
    "      0.9784279, 1.0221335]]\n",
    "])\n",
    "\n",
    "print('error: ', rel_error(expected_output, output.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1msDyvLseyI"
   },
   "source": [
    "Take a look at the `VisionTransformer` implementation in `cs231n/classifiers/transformer.py`.\n",
    "\n",
    "For classification, ViT divides the input image into patches and processes the sequence of patch vectors using a transformer. Finally, all the patch vectors are average-pooled and used to predict the image class. We will use the same 1D sinusoidal positional encoding to inject ordering information, though 2D sinusoidal and learned positional encodings are also valid choices.\n",
    "\n",
    "Complete the ViT forward pass and test it below. You should see relative error less than 1e-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AHZ1LMuRpOiV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores error:  0.5000661674544938\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "from cs231n.classifiers.transformer import VisionTransformer\n",
    "\n",
    "imgs = torch.randn(3, 3, 32, 32)\n",
    "transformer = VisionTransformer()\n",
    "scores = transformer(imgs)\n",
    "expected_scores = np.asarray(\n",
    "    [[-0.13013132,  0.13652277, -0.04656096, -0.16443546, -0.08946665,\n",
    "        -0.10123537,  0.11047452,  0.01317241,  0.17256221,  0.16230097],\n",
    "       [-0.11988413,  0.20006064, -0.04028708, -0.06937674, -0.07828291,\n",
    "        -0.13545093,  0.18698244,  0.01878054,  0.14309685,  0.03245382],\n",
    "       [-0.11540816,  0.21416159, -0.07740889, -0.08336161, -0.1645808 ,\n",
    "        -0.12318538,  0.18035144,  0.05492767,  0.15997584,  0.12134959]])\n",
    "print('scores error: ', rel_error(expected_scores, scores.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Redma2A5BZA"
   },
   "source": [
    "\n",
    "We will first verify our implementation by overfitting it on one training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Myf9A3UK4TpM"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = CIFAR10(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = CIFAR10(root='data', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YU2pQigB9kxp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100] Loss 2.375001, Top-1 Accuracy: 0.031\n",
      "[10/100] Loss 2.112257, Top-1 Accuracy: 0.250\n",
      "[20/100] Loss 1.915519, Top-1 Accuracy: 0.328\n",
      "[30/100] Loss 1.668079, Top-1 Accuracy: 0.359\n",
      "[40/100] Loss 1.424713, Top-1 Accuracy: 0.516\n",
      "[50/100] Loss 1.002456, Top-1 Accuracy: 0.672\n",
      "[60/100] Loss 0.754262, Top-1 Accuracy: 0.625\n",
      "[70/100] Loss 0.375258, Top-1 Accuracy: 0.922\n",
      "[80/100] Loss 0.139977, Top-1 Accuracy: 1.000\n",
      "[90/100] Loss 0.050487, Top-1 Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 1e-3  # Experiment with this\n",
    "weight_decay = 1.e-4  # Experiment with this\n",
    "\n",
    "\n",
    "batch = next(iter(DataLoader(train_data, batch_size=64, shuffle=False)))\n",
    "model = VisionTransformer(dropout=0.0)\n",
    "loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "model.train()\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    imgs, target = batch\n",
    "    out = model(imgs)\n",
    "    loss = loss_criterion(out, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    top1 = (out.argmax(-1) == target).float().mean().item()\n",
    "    if epoch % 10 == 0:\n",
    "      print(f\"[{epoch}/{epochs}] Loss {loss.item():.6f}, Top-1 Accuracy: {top1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8mrgWcfEE8XE",
    "test": "vit_overfit_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting ViT on one batch. Top-1 accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# You should get perfect 1.00 accuracy\n",
    "print(f\"Overfitting ViT on one batch. Top-1 accuracy: {top1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI5-AVJNGYS9"
   },
   "source": [
    "Now we will train it on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9W2dUKrz8MbG"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bdfa92becc4519a1d32de52954bf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6273665672dc4927bb256eb8f72ee99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3481c72989234503be46555e4df0b63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d069b7861a52415088525c63d8803be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cs231n.classification_solver_vit import ClassificationSolverViT\n",
    "\n",
    "############################################################################\n",
    "# TODO: Train a Vision Transformer model that achieves over 0.45 test      #\n",
    "# accuracy on CIFAR-10 after 2 epochs by adjusting the model architecture  #\n",
    "# and/or training parameters as needed.                                    #\n",
    "#                                                                          #\n",
    "# Note: If you want to use a GPU runtime, go to `Runtime > Change runtime  #\n",
    "# type` and set `Hardware accelerator` to `GPU`. This will reset Colab,    #\n",
    "# so make sure to rerun the entire notebook from the beginning afterward.  #\n",
    "############################################################################\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.0\n",
    "batch_size = 128\n",
    "model = VisionTransformer(img_size=32, patch_size=4, in_channels=3,\n",
    "                 embed_dim=128, num_layers=6, num_heads=4,\n",
    "                 dim_feedforward=256, num_classes=10, dropout=0.1)  # You may want to change the default params.\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "solver = ClassificationSolverViT(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    model=model,\n",
    "    num_epochs = 2,  # Don't change this\n",
    "    learning_rate = learning_rate,\n",
    "    weight_decay = weight_decay,\n",
    "    batch_size = batch_size,\n",
    ")\n",
    "\n",
    "solver.train('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "X1xe22cuGsnw",
    "test": "vit_test_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.4842\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on test set: {solver.results['best_test_acc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtjQqsDBm9or"
   },
   "source": [
    "# Inline Question 2\n",
    "\n",
    "Despite their recent success in large-scale image recognition tasks, ViTs often lag behind traditional CNNs when trained on smaller datasets. What underlying factor contribute to this performance gap? What techniques can be used to improve the performance of ViTs on small datasets?\n",
    "\n",
    "尽管视觉Transformer（ViTs）最近在大规模图像识别任务中取得了成功，但在较小的数据集上进行训练时，它们往往落后于传统的卷积神经网络（CNNs）。是什么潜在因素导致了这种性能差距？可以使用哪些技术来提高视觉Transformer在小数据集上的性能？\n",
    "\n",
    "**Your Answer**: fill this in.\n",
    "\n",
    "CNN先天具备强先验假设，例如局部性和平移等变性，因此初期学习效率更高。\n",
    "\n",
    "提高性能的方法：预训练+迁移学习，自监督学习，CNN+ViT结合，数据增强等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cau-UmPBJ3ez"
   },
   "source": [
    "# Inline Question 3\n",
    "\n",
    "How does the computational cost of the self-attention layers in a ViT change if we independently make the following changes?\n",
    "\n",
    "(i) Double the hidden dimension.\n",
    "(ii) Double the height and width of the input image.\n",
    "(iii) Double the patch size.\n",
    "(iv) Double the number of layers.\n",
    "\n",
    "**Your Answer**: fill this in.\n",
    "\n",
    "自注意力机制的计算复杂度约为 $O(S \\cdot D^2 + S^2 \\cdot D)$，其中：\n",
    "* $S$ 是 Patch 的数量 (即序列长度)。\n",
    "* $D$ 是每个 Patch 的隐藏维度 (Hidden Dimension)。\n",
    "* 在我们的模型中，D>>S，因此计算复杂度约为$O(S \\cdot D^2)$\n",
    "\n",
    "\n",
    "**(i) 隐藏维度加倍 (D → 2D)**\n",
    "\n",
    "* 成本变为 $O(S \\cdot (2D)^2) = 4 \\cdot O(S \\cdot D^2)$。\n",
    "* **答案**: 计算成本大约**x4**。\n",
    "\n",
    "\n",
    "**(ii) 输入图像的高度和宽度加倍 (H → 2H, W → 2W)**\n",
    "\n",
    "* Patch 数量 $S = (H/P) \\times (W/P)$。\n",
    "* 新的 Patch 数量 $S' = (2H/P) \\times (2W/P) = 4 \\times (H \\cdot W / P^2) = 4S$。\n",
    "* 成本变为 $O((4S) \\cdot D^2) 4 \\cdot O(S \\cdot D^2)$。\n",
    "* **答案**: 计算成本增加到原来的**4倍**。\n",
    "\n",
    "\n",
    "**(iii) Patch 大小加倍 (P → 2P)**\n",
    "\n",
    "* Patch 数量 $N = (H/P) \\times (W/P)$。\n",
    "* 新的 Patch 数量 $N' = (H/2P) \\times (W/2P) = \\frac{1}{4} \\times (H \\cdot W / P^2) = N/4$。\n",
    "* **答案**: 计算成本降低到原来的**4分之一**。\n",
    "\n",
    "\n",
    "**(iv) 层数加倍 (L → 2L)**\n",
    "\n",
    "* 这个变化影响的是整个 Transformer Encoder 的总成本，而不是单个自注意力层的成本。\n",
    "* 自注意力层的总成本与层数成**线性关系**。\n",
    "* **答案**: 自注意力层的总计算成本**翻倍 (x2)**。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs231n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
