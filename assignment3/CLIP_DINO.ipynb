{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "S7MV4XqiiQ0g"
            },
            "outputs": [],
            "source": [
                "# # This mounts your Google Drive to the Colab VM.\n",
                "# from google.colab import drive\n",
                "# drive.mount('/content/drive', force_remount=True)\n",
                "\n",
                "# # TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
                "# # assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
                "# FOLDERNAME = \"cs231n/assignments/assignment3/\"\n",
                "# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
                "\n",
                "# # Now that we've mounted your Drive, this ensures that\n",
                "# # the Python interpreter of the Colab VM can load\n",
                "# # python files from within it.\n",
                "# import sys\n",
                "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "U0CPS6Gm27O8"
            },
            "outputs": [],
            "source": [
                "# This downloads the COCO dataset to your Drive if it doesn't already exist\n",
                "# (you should already have this dataset from a previous notebook!)\n",
                "# Uncomment the following if you don't have it.\n",
                "# %cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
                "# !bash get_coco_captioning.sh\n",
                "# %cd /content/drive/My\\ Drive/$FOLDERNAME"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "7mkg1F0J081A"
            },
            "outputs": [],
            "source": [
                "# Some useful python libraries\n",
                "# ! pip install ftfy regex tqdm\n",
                "# ! pip install git+https://github.com/openai/CLIP.git\n",
                "# ! pip install decord"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "k-IumfyxjUZT"
            },
            "source": [
                "# State-of-the-Art Pretrained Image Models\n",
                "\n",
                "In the previous exercise, you learned about [SimCLR](https://arxiv.org/abs/2002.05709) and how contrastive self-supervised learning can be used to learn meaningful image representations. In this notebook, we will explore two more recent models that also aim to learn high-quality visual representations and have demonstrated strong and robust performance on a variety of downstream tasks.\n",
                "\n",
                "\n",
                "First, we will examine the [CLIP](https://github.com/openai/CLIP) model. Like SimCLR, CLIP uses a contrastive learning objective, but instead of contrasting two augmented views of the same image, it contrasts two different modalities: text and image. To train CLIP, OpenAI collected a large dataset of ~400M image-text pairs from the internet, including sources like Wikipedia and image alt text. The resulting model learns rich, high-level image features and has achieved impressive zero-shot performance on many vision benchmarks.\n",
                "\n",
                "Next, we will explore [DINO](https://github.com/facebookresearch/dino), a self-supervised learning method for vision tasks that applies contrastive learning in a self-distillation framework with multi-crop augmentation strategy. The authors showed that the features learned by DINO ViTs are fine-grained and semantically rich with explicit information about the semantic segmentation of the image.\n",
                "\n",
                "# 最先进的预训练图像模型\n",
                "\n",
                "在上一个练习中，你了解了[SimCLR](https://arxiv.org/abs/2002.05709)以及对比自监督学习如何用于学习有意义的图像表征。在本笔记本中，我们将探讨另外两个较新的模型，它们也旨在学习高质量的视觉表征，并在各种下游任务中表现出强大且稳健的性能。\n",
                "\n",
                "首先，我们将研究[CLIP](https://github.com/openai/CLIP)模型。与SimCLR一样，CLIP使用对比学习目标，但它不是对比同一图像的两个增强视图，而是对比两种不同的模态：文本和图像。为了训练CLIP，OpenAI从互联网上收集了一个包含约4亿个图像-文本对的大型数据集，来源包括维基百科和图像替代文本等。由此产生的模型学习到了丰富、高级的图像特征，并在许多视觉基准测试中取得了令人印象深刻的零样本性能。\n",
                "\n",
                "接下来，我们将探讨[DINO](https://github.com/facebookresearch/dino)，这是一种用于视觉任务的自监督学习方法，它在自蒸馏框架中应用对比学习，并采用多裁剪增强策略。作者们表明，DINO ViT学到的特征具有细粒度和丰富的语义，包含关于图像语义分割的明确信息。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "JZJOAUKaoze_"
            },
            "source": [
                "# CLIP\n",
                "\n",
                "As explained above, CLIP's training objective incorporates both text and images, building upon the principles of contrastive learning. Consider this quote from the SimCLR notebook:\n",
                ">The goal of the contrastive loss is to maximize agreement between the final vectors **$z_i = g(h_i)$** and **$z_j = g(h_j)$**.\n",
                "\n",
                "Similarly, CLIP is trained to maximize agreement between two vectors. However, because these vectors come from different modalities, CLIP uses two separate encoders: a transformer-based Text Encoder and a Vision Transformer (ViT)-based Image Encoder. Note that some smaller, more efficient versions of CLIP use a ResNet as the Image Encoder instead of a ViT.\n",
                "\n",
                "Run the cell below to visualize the training and inference pipeline of CLIP.\n",
                "\n",
                "During the pretraining phase, each batch consists of multiple images along with their corresponding captions. Each image is independently processed by an Image Encoder—typically a visual model like a Vision Transformer (ViT) or a Convolutional Neural Network (ConvNet)—which produces an image embedding $I_n$. Likewise, each caption is independently processed by a Text Encoder to generate a corresponding text embedding $T_n$. Next, we compute the pairwise similarities between all image-text combinations, meaning each image is compared with every caption, and vice versa. The training objective is to maximize the similarity scores along the diagonal of the resulting similarity matrix -- that is, the scores for the matching image-caption pairs $(I_n, T_n)$.  Through backpropagation, the model learns to assign higher similarity scores to true matches than to mismatched pairs.\n",
                "\n",
                "Through this setup, CLIP effectively learns to represent images and texts in a shared latent space. In this space, semantic concepts are encoded in a modality-independent way, enabling meaningful cross-modal comparisons between visual and textual inputs.\n",
                "\n",
                "如上所述，CLIP的训练目标融合了文本和图像，其基础是对比学习的原理。不妨看看SimCLR笔记本中的这句话：\n",
                ">对比损失的目标是最大化最终向量 **$z_i = g(h_i)$** 和 **$z_j = g(h_j)$** 之间的一致性。\n",
                "\n",
                "同样，CLIP经过训练以最大化两个向量之间的一致性。不过，由于这些向量来自不同的模态，CLIP使用了两个独立的编码器：一个基于Transformer的文本编码器和一个基于视觉Transformer（ViT）的图像编码器。需要注意的是，一些更小、更高效的CLIP版本使用ResNet作为图像编码器，而非ViT。\n",
                "\n",
                "运行下面的单元格，以可视化CLIP的训练和推理流程。\n",
                "\n",
                "在预训练阶段，每个批次都包含多张图像及其对应的标题。每张图像由图像编码器独立处理——该编码器通常是视觉Transformer（ViT）或卷积神经网络（ConvNet）之类的视觉模型，处理后会生成图像嵌入$I_n$。同样，每个标题由文本编码器独立处理，以生成相应的文本嵌入$T_n$。接下来，我们计算所有图像-文本组合之间的成对相似度，这意味着每张图像会与每个标题进行比较，反之亦然。训练目标是最大化所得相似度矩阵对角线上的相似度分数——也就是匹配的图像-标题对$(I_n, T_n)$的分数。通过反向传播，模型学会给真实匹配对分配比不匹配对更高的相似度分数。\n",
                "\n",
                "通过这种设置，CLIP有效地学习到了在一个共享的潜在空间中表示图像和文本。在这个空间中，语义概念以与模态无关的方式进行编码，从而能够对视觉输入和文本输入进行有意义的跨模态比较。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "LyNGHPpf4kIL"
            },
            "outputs": [],
            "source": [
                "from IPython.display import Image as ColabImage\n",
                "ColabImage(f'CLIP.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "96UkICCbmoX-"
            },
            "source": [
                "**Inline Question 1** -\n",
                "\n",
                "Why does CLIP's learning depend on the batch size? If the batch size is fixed, what strategy can we use to learn rich image features?\n",
                "\n",
                "$\\color{blue}{\\textit Your Answer:}$\n",
                "\n",
                "因为CLIP训练过程类似于一个分类数为batch_size的多分类任务，如果batch size过小，就会导致模型训练难度小，模型学不到更精细的特征，训练效率低下。   \n",
                "如果batch size是固定的，那么可以在一个batch当中人为地加入容易混淆的相似样本，增大训练难度。\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lkVe0nRuxXIP"
            },
            "source": [
                "# Loading COCO dataset\n",
                "\n",
                "We'll use the same captioning dataset you used to train your RNN captioning model, but instead of generating the captions lets see if we can match each image to the correct caption."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "IUkEEQ2YyTSo"
            },
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import time, os, json\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import clip\n",
                "import torch\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "from PIL import Image\n",
                "from cs231n.clip_dino import *\n",
                "\n",
                "def rel_error(x, y):\n",
                "    \"\"\"Returns relative error.\"\"\"\n",
                "    return np.max(np.abs(x - y) / (np.maximum(1e-10, np.abs(x) + np.abs(y))))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "0jq0KZifyNDA"
            },
            "outputs": [],
            "source": [
                "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
                "from cs231n.image_utils import image_from_url"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "AT-Ux6N8ovqZ"
            },
            "outputs": [],
            "source": [
                "# Load COCO data from disk into a dictionary.\n",
                "# this is the same dataset you used for the RNN captioning notebook :)\n",
                "data = load_coco_data(pca_features=True)\n",
                "\n",
                "# Print out all the keys and values from the data dictionary.\n",
                "for k, v in data.items():\n",
                "    if type(v) == np.ndarray:\n",
                "        print(k, type(v), v.shape, v.dtype)\n",
                "    else:\n",
                "        print(k, type(v), len(v))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "aZYimNYozKxh"
            },
            "outputs": [],
            "source": [
                "# we're just using the loaded captions from COCO, so we need to decode them and get rid of the special tokens.\n",
                "decoded_captions= []\n",
                "for caption in data['val_captions']:\n",
                "  caption = decode_captions(caption, data['idx_to_word'])\\\n",
                "    .replace('<START>', '')\\\n",
                "    .replace('<END>', '')\\\n",
                "    .replace('<UNK>', '')\\\n",
                "    .strip()\n",
                "  decoded_captions.append(caption)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "-bdpT8K52ukP"
            },
            "outputs": [],
            "source": [
                "# lets get 10 examples\n",
                "mask = np.array([135428, 122586, 122814, 133173, 176639, 163828,  98169,   6931,\n",
                "        19488, 175760])\n",
                "first_captions = [decoded_captions[elem] for elem in mask]\n",
                "\n",
                "img_idxs = data['val_image_idxs'][mask]       # the images the captions refer to\n",
                "first_images   = [image_from_url(data['val_urls'][j]) for j in img_idxs]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "n-Zvmbg_N42T"
            },
            "outputs": [],
            "source": [
                "for i, (caption, image) in enumerate(zip(first_captions, first_images)):\n",
                "    plt.imshow(image)\n",
                "    plt.axis('off')\n",
                "    caption_str = caption\n",
                "    plt.title(caption_str)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "I144vQb4sXVW"
            },
            "source": [
                "# Running the CLIP Model\n",
                "\n",
                "First we'll use the pretrained CLIP model to extract features from the texts and images separetely."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "aNWKDxLs29Dd"
            },
            "outputs": [],
            "source": [
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "F-inoZiP79TJ"
            },
            "outputs": [],
            "source": [
                "# You can check the model layers by printing the model.\n",
                "# CLIP's model code is available at https://github.com/openai/CLIP/tree/main/clip\n",
                "# print(clip_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "znkbk9r1zjkC"
            },
            "outputs": [],
            "source": [
                "# First, we encode the captions into vectors in the shared embedding space.\n",
                "# Since we're using a Transformer as the text encoder, we need to tokenize the text first.\n",
                "text_tokens = clip.tokenize(first_captions).to(device)\n",
                "with torch.no_grad():\n",
                "    text_features = clip_model.encode_text(text_tokens)\n",
                "\n",
                "# Sanity check, print the shape\n",
                "print(text_features.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "8vvRlMrR2XZk"
            },
            "outputs": [],
            "source": [
                "# Then, we encode the images into the same embedding space.\n",
                "processed_images = [\n",
                "    clip_preprocess(Image.fromarray(img)).unsqueeze(0)\n",
                "    for img in first_images\n",
                "]\n",
                "images_tensor = torch.cat(processed_images, dim=0).to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    image_features = clip_model.encode_image(images_tensor)\n",
                "\n",
                "# sanity check, print the shape\n",
                "print(image_features.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "G3IfwGD__Xb8"
            },
            "source": [
                "Open `cs231n/clip_dino.py` and implement `get_similarity_no_loop` to compute similarity scores between text features and image features. Test your implementation below, you should see relative errors less than 1e-5."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "UWRW5AG4_X94"
            },
            "outputs": [],
            "source": [
                "from cs231n.clip_dino import get_similarity_no_loop\n",
                "torch.manual_seed(231)\n",
                "np.random.seed(231)\n",
                "M, N, D = 5, 6, 10\n",
                "\n",
                "test_text_features = torch.randn(N, D)\n",
                "test_image_features = torch.randn(M, D)\n",
                "out = get_similarity_no_loop(test_text_features, test_image_features)\n",
                "\n",
                "expected_out = np.array([\n",
                "    [ 0.1867811 , -0.23494351,  0.44155994, -0.18950461,  0.00100103],\n",
                "    [ 0.17905031, -0.25469488, -0.64330417,  0.25097957, -0.09327742],\n",
                "    [-0.4407011 , -0.4365381 ,  0.32857686, -0.3765278 ,  0.01049389],\n",
                "    [ 0.24815483,  0.42157224, -0.08459304,  0.14132318, -0.26935193],\n",
                "    [ 0.02309848, -0.01441101,  0.5469337 ,  0.6018773 ,  0.21581158],\n",
                "    [ 0.41579214, -0.014449  , -0.7242257 ,  0.39348006,  0.0822239 ],\n",
                "]).astype(np.float32)\n",
                "\n",
                "print(\"relative error: \", rel_error(out.numpy(), expected_out))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "8t54B7xh4PL-"
            },
            "outputs": [],
            "source": [
                "# Let's visualize the similarities between our batch of images and their captions.\n",
                "\n",
                "similarities = get_similarity_no_loop(text_features, image_features).cpu().detach().numpy()\n",
                "\n",
                "plt.figure(figsize=(20, 14))\n",
                "plt.imshow(similarities, vmin=0.1, vmax=0.3)\n",
                "plt.yticks(range(len(text_features)), first_captions, fontsize=18)\n",
                "plt.xticks([])\n",
                "for i, image in enumerate(first_images):\n",
                "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
                "for x in range(similarities.shape[1]):\n",
                "    for y in range(similarities.shape[0]):\n",
                "        plt.text(x, y, f\"{similarities[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
                "\n",
                "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
                "    plt.gca().spines[side].set_visible(False)\n",
                "\n",
                "plt.xlim([-0.5, len(image_features) - 0.5])\n",
                "plt.ylim([len(text_features) + 0.5, -2])\n",
                "\n",
                "plt.title(\"Cosine similarity between text and image features\", size=20)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "JnNuJaABp8Nl"
            },
            "source": [
                "# Zero Shot Classifier\n",
                "\n",
                "You will be able to see a high similarity between matching image-caption pairs above. We can leverage this property to design an image classifier that doesn't require any labeled data (i.e., a zero-shot classifier). Each class can be represented using an appropriate natural language description, and any input image will be classified into the class whose description has the highest similarity with the image in CLIP's embedding space."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2lDbkKaQY8Gd"
            },
            "source": [
                "Implement `clip_zero_shot_classifier` in `cs231n/clip_dino.py` and test it below. You should be able to see the following predictions:\n",
                "\n",
                "['a person', 'an animal', 'an animal', 'food', 'a person', 'a landscape', 'other', 'other', 'other', 'a person']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "bM_WEqsYrCx9"
            },
            "outputs": [],
            "source": [
                "from cs231n.clip_dino import clip_zero_shot_classifier\n",
                "\n",
                "classes = [\"a person\", \"an animal\", \"food\", \"a landscape\", \"other\"]\n",
                "\n",
                "pred_classes = clip_zero_shot_classifier(\n",
                "    clip_model, clip_preprocess, first_images, classes, device)\n",
                "\n",
                "print(pred_classes)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "yW5xwHdWZlzl"
            },
            "source": [
                "Run the cell below to visualize the predictions. As you can see, CLIP offers a straightforward way to perform reasonable zero-shot classification across any class taxonomy.\n",
                "\n",
                "CLIP was the first model to outperform standard supervised training on ImageNet classification without using any ImageNet images or labels (The original CLIP paper has many such interesting experiments and analysis).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "om4mnnVXZKAL"
            },
            "outputs": [],
            "source": [
                "# Visualize the zero shot predictions\n",
                "for i, (pred_class, image) in enumerate(zip(pred_classes, first_images)):\n",
                "    plt.imshow(image)\n",
                "    plt.axis('off')\n",
                "    plt.title(pred_class)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zVCDz3tyfaAD"
            },
            "source": [
                "# Image Retrieval using CLIP\n",
                "\n",
                "Just as we used CLIP to retrieve the matching class name for each image, we can also use it to retrieve matching images from text inputs (semantic image retrieval). Implement the `CLIPImageRetriever` in `cs231n/clip_dino.py` and test it by running the two cells below. The expected top 2 outputs for each query are provided in the comments.\n",
                "\n",
                "正如我们使用CLIP为每个图像检索匹配的类别名称一样，我们也可以使用它从文本输入中检索匹配的图像（语义图像检索）。在`cs231n/clip_dino.py`中实现`CLIPImageRetriever`，并通过运行下面的两个单元格进行测试。每个查询的预期前2名输出已在注释中提供。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "077RtVSAfaXd"
            },
            "outputs": [],
            "source": [
                "from cs231n.clip_dino import CLIPImageRetriever\n",
                "clip_retriever = CLIPImageRetriever(clip_model, clip_preprocess, first_images, device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "609IYhTzkMBF"
            },
            "outputs": [],
            "source": [
                "# query = \"sports\"  # tennis, skateboard\n",
                "query = \"black and white\"  # bathroom, zerbas\n",
                "img_indices = clip_retriever.retrieve(query)\n",
                "\n",
                "for img_index in img_indices:\n",
                "    plt.imshow(first_images[img_index])\n",
                "    plt.axis('off')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "z1gVu2VDogLb"
            },
            "source": [
                "**Inline Question 2** -\n",
                "\n",
                "CLIP learns to align image and text representations in a shared latent space using a contrastive loss. How would you extend this idea to more than two modalities?\n",
                "\n",
                "$\\color{blue}{\\textit Your Answer:}$\n",
                "\n",
                "可以在多种模态之间进行对比学习，每一种模态都使用独立的编码器，这些编码器输出维度相同的特征向量，然后在一个batch里面两两配对计算对比损失。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "otX-grqhmRAN"
            },
            "source": [
                "# DINO\n",
                "\n",
                "As mentioned earlier, models trained with vanilla contrastive learning methods such as SimCLR and CLIP require very large batch sizes. This makes them computationally expensive and limits their accessibility. Subsequent works, like [BYOL](https://arxiv.org/abs/2006.07733), propose an alternative approach that avoids the need for numerous negative samples by using a student-teacher framework. This method performs surprisingly well and was later adopted by [DINO](https://arxiv.org/abs/2104.14294) .\n",
                "\n",
                "Similar to SimCLR, DINO is trained to maximize the agreement between two vectors derived from different views of the same image. However, unlike SimCLR, DINO uses two separate encoders which are trained differently. The student network is updated via backpropagation to match the outputs of the teacher network. The teacher network is not updated via backpropagation; instead, its weights are updated using an exponential moving average (EMA) of the student's weights. This means that the teacher model evolves more slowly and provides a stable target for the student to learn from.\n",
                "\n",
                "Run the cell below to visualize the DINO training pipeline.\n",
                "\n",
                "如前所述，使用SimCLR和CLIP等常规对比学习方法训练的模型需要非常大的批量大小。这使得它们计算成本高昂，并且限制了其可及性。后续的研究工作，如[BYOL](https://arxiv.org/abs/2006.07733)，提出了一种替代方法，通过使用学生-教师框架来避免对大量负样本的需求。这种方法的表现出奇地好，后来被[DINO](https://arxiv.org/abs/2104.14294) 所采用。\n",
                "\n",
                "与SimCLR类似，DINO的训练目标是最大化从同一图像的不同视图中得到的两个向量之间的一致性。然而，与SimCLR不同的是，DINO使用两个独立的编码器，且这两个编码器的训练方式不同。学生网络通过反向传播进行更新，以匹配教师网络的输出。教师网络不通过反向传播进行更新，相反，其权重是利用学生权重的指数移动平均（EMA）来更新的。这意味着教师模型的进化速度更慢，并为学生提供了一个稳定的学习目标。\n",
                "\n",
                "运行下面的单元格以可视化DINO的训练流程。\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "35APMU0nq0Ja"
            },
            "outputs": [],
            "source": [
                "from IPython.display import Image as ColabImage\n",
                "ColabImage(f'dino.gif')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "FuXuvIq8th9M"
            },
            "outputs": [],
            "source": [
                "# first let's get rid of the CLIP model that's currently using memory\n",
                "del clip_model\n",
                "# Uncomment the following if you are using GPU runtime\n",
                "# torch.cuda.empty_cache()\n",
                "# torch.cuda.ipc_collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "NBM480L42ykK"
            },
            "outputs": [],
            "source": [
                "# Load smallest dino model. ViT-S/8. Here ViT-S has ~22M parameters and\n",
                "# works on 8x8 patches.\n",
                "dino_model = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')\n",
                "dino_model.eval().to(device)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "-35sU9tQ35KA"
            },
            "outputs": [],
            "source": [
                "# the image we will be playing around with\n",
                "sample_image = Image.fromarray(first_images[0]).convert(\"RGB\")\n",
                "sample_image"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "jQ-tH24e6P0D"
            },
            "source": [
                "# DINO Attention Maps\n",
                "\n",
                "Since the loaded DINO checkpoint is based on the ViT architecture, we can visualize what each attention head is focusing on. The code below generates heatmaps showing which patches of the original image the [CLS] token attends to across the various heads in the final layer. Although this model was trained using a self-supervised objective without any explicit instruction to recognize \"structure\" in images, still...\n",
                "\n",
                "由于加载的DINO检查点基于ViT架构，我们可以可视化每个注意力头所关注的内容。下面的代码生成热图，展示在最后一层的各个头中，[CLS]标记关注原始图像的哪些块。尽管该模型是使用自监督目标训练的，没有任何明确的指令来识别图像中的“结构”，但仍然……\n",
                "\n",
                "Do you notice any patterns?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "ux-p68vw4fdt"
            },
            "outputs": [],
            "source": [
                "# Preprocess\n",
                "from torchvision import transforms as T\n",
                "transform = T.Compose([\n",
                "    T.Resize((480, 480)),\n",
                "    T.ToTensor(),\n",
                "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
                "])\n",
                "img_tensor = transform(sample_image)\n",
                "w, h = img_tensor.shape[1:]\n",
                "img_tensor = img_tensor[None].to(device)\n",
                "\n",
                "# Extract attention\n",
                "with torch.no_grad():\n",
                "    attn = dino_model.get_last_selfattention(img_tensor)[0, :, 0, 1:]\n",
                "nh, tokens = attn.shape\n",
                "w_feat, h_feat = w // 8, h // 8\n",
                "attn = attn.reshape(nh, w_feat, h_feat)\n",
                "attn = torch.nn.functional.interpolate(attn.unsqueeze(0), scale_factor=8, mode=\"nearest\")[0].cpu().numpy()\n",
                "\n",
                "# Plot attention heads\n",
                "fig, axes = plt.subplots(1, nh, figsize=(3 * nh, 3))\n",
                "for i in range(nh):\n",
                "    ax = axes[i] if nh > 1 else axes\n",
                "    ax.imshow(attn[i], cmap='inferno')\n",
                "    ax.axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "aMPaTwfB9e3Y"
            },
            "outputs": [],
            "source": [
                "# Extract patch token features and discard [CLS] token.\n",
                "with torch.no_grad():\n",
                "    all_tokens = dino_model.get_intermediate_layers(img_tensor, n=1)[0]  # (1, 1+N, D)\n",
                "    patch_tokens = all_tokens[:, 1:, :]  # (N, D)\n",
                "\n",
                "print(img_tensor.shape)\n",
                "print(all_tokens.shape)\n",
                "print(patch_tokens.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "6zQgCDpQ9r3p"
            },
            "source": [
                "\n",
                "**Inline Question 3**\n",
                "\n",
                "How do we get the tensor shapes printed above? Explain your answer.\n",
                "\n",
                "$\\color{blue}{\\textit Your Answer:}$\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "xG19KjdGCGp-"
            },
            "source": [
                "# DINO Features\n",
                "\n",
                "To understand what the model is encoding in each patch, we can visualize the contents of each patch token. Since these embeddings are high-dimensional and difficult to interpret directly, we'll use PCA to identify the directions of highest variance in the feature space.\n",
                "\n",
                "In the next cell, we visualize the three principal directions of variance in the feature space. This reveals the dominant structure that the patch embeddings are capturing.\n",
                "\n",
                "为了理解模型在每个补丁中编码了什么，我们可以将每个补丁标记的内容可视化。由于这些嵌入是高维的，难以直接解释，我们将使用主成分分析（PCA）来识别特征空间中方差最大的方向。\n",
                "\n",
                "在下一个单元格中，我们将可视化特征空间中三个主要的方差方向。这将揭示补丁嵌入所捕捉到的主要结构。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "-YKYFpoa_XpS"
            },
            "outputs": [],
            "source": [
                "from sklearn.decomposition import PCA\n",
                "\n",
                "np.random.seed(231)\n",
                "\n",
                "# PCA\n",
                "pca = PCA(n_components=3)\n",
                "patch_pca = pca.fit_transform(patch_tokens.cpu().numpy()[0])\n",
                "\n",
                "# Normalize PCA components to [0, 1] for RGB display\n",
                "patch_rgb = (patch_pca - patch_pca.min(0)) / (patch_pca.max(0) - patch_pca.min(0))\n",
                "\n",
                "# Reshape to image grid (60x60, 3)\n",
                "patch_rgb_img = patch_rgb.reshape(60, 60, 3)\n",
                "\n",
                "# Show as image\n",
                "plt.figure(figsize=(6, 6))\n",
                "plt.imshow(patch_rgb_img)\n",
                "plt.axis('off')\n",
                "plt.title(\"Patch Embeddings (PCA → RGB)\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "tgQR6poMAyOK"
            },
            "source": [
                "**Inline Question 4** -\n",
                "\n",
                "What kind of structure do you see in the visualization above? What does it imply when a region consistently appears in a specific color? What does it mean when two regions have distinctly different color? Remember that PCA reveals the directions of highest variance in the feature space across all patches. A patch's color reflects its distinct feature content.\n",
                "\n",
                "你在上面的可视化中看到了什么样的结构？当一个区域始终呈现特定颜色时，这意味着什么？当两个区域颜色明显不同时，又意味着什么？请记住，主成分分析（PCA）揭示了所有斑块在特征空间中方差最大的方向。一个斑块的颜色反映了其独特的特征内容。\n",
                "\n",
                "$\\color{blue}{\\textit Your Answer:}$\n",
                "\n",
                "DINO在自监督学习中自动学会了语义分割的能力，属于特定物体的区域呈现出相似颜色，不同物体呈现出不同颜色。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5J7GZfBPFyFQ"
            },
            "source": [
                "# A Simple Segmentation Model over DINO Features\n",
                "\n",
                "In the previous section, we saw that DINO features can provide surprisingly good segmentation cues. Now, let's put that idea to the test by training a simple segmentation model on the [DAVIS dataset](https://davischallenge.org). The DAVIS dataset (Densely Annotated VIdeo Segmentation) was created for video object segmentation tasks. It provides frame-by-frame, pixel-level annotations of objects within videos. For this experiment, we'll train our model using the annotations from just a single frame of a video and see how well it performs on the remaining frames of the same.\n",
                "\n",
                "Our model will be intentionally minimal: we'll extract DINO features per patch and train a lightweight per-patch classifier using only the patches from that one annotated frame. Typically, you would train on the full dataset and evaluate on a separate validation set containing different videos. But here, we will test the one-shot capabilities of DINO features.\n",
                "\n",
                "在上一节中，我们了解到DINO特征能够提供出人意料的良好分割线索。现在，让我们通过在[DAVIS数据集](https://davischallenge.org)上训练一个简单的分割模型来验证这一想法。DAVIS数据集（密集标注视频分割数据集）是为视频目标分割任务创建的。它提供了视频中目标的逐帧、像素级标注。在这个实验中，我们将仅使用一个视频的单帧标注来训练模型，并观察其在该视频其余帧上的表现。\n",
                "\n",
                "我们的模型会特意设计得非常简洁：我们将提取每个patch的DINO特征，并仅使用那个带标注帧的patch来训练一个轻量级的逐patch分类器。通常情况下，你会在完整数据集上进行训练，并在包含不同视频的独立验证集上进行评估。但在这里，我们将测试DINO特征的单样本学习能力。\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "h6aAbgondcy2"
            },
            "outputs": [],
            "source": [
                "from cs231n.clip_dino import DavisDataset\n",
                "\n",
                "# A helper class to work with DAVIS dataset.\n",
                "# It may take ~5 minutes on the first run of this cell to download the dataset.\n",
                "davis_ds = DavisDataset()\n",
                "\n",
                "# Get a specific test video. Do NOT change this for submission.\n",
                "frames, masks = davis_ds.get_sample(7)\n",
                "num_classes = masks.max() + 1\n",
                "\n",
                "print(frames.shape, masks.shape, num_classes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Yl0spxJRiAjf"
            },
            "outputs": [],
            "source": [
                "# Get DINO patch features and corresponding class labels for a middle frame\n",
                "train_fi = 40\n",
                "X_train = davis_ds.process_frames(frames[train_fi:train_fi+1], dino_model, device)[0]\n",
                "Y_train = davis_ds.process_masks(masks[train_fi:train_fi+1], device)[0]\n",
                "print(X_train.shape, Y_train.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "w6S3DAKl_9NE"
            },
            "source": [
                "Complete the implementation of the `DINOSegmentation` class in `cs231n/clip_dino.py`, and test it by running the two cells below. You should achieve a mean IoU greater than 0.45 on the first test frame and greater than 0.50 on the last test frame. To prevent overfitting on the training patch features, consider designing a very lightweight model (e.g., a linear layer or a 2-layer MLP) and applying appropriate weight decay.\n",
                "\n",
                "You may use GPU runtime to speed up training and evaluation. Make sure to rerun the entire notebook if you change runtime type.\n",
                "\n",
                "完成`cs231n/clip_dino.py`中`DINOSegmentation`类的实现，并通过运行下面的两个单元格进行测试。在第一个测试帧上，你的平均交并比（mean IoU）应大于0.45，在最后一个测试帧上应大于0.50。为了防止在训练补丁特征上发生过拟合，可以考虑设计一个非常轻量级的模型（例如，一个线性层或两层的多层感知器（MLP））并应用适当的权重衰减。\n",
                "\n",
                "你可以使用GPU运行时来加快训练和评估速度。如果你更改了运行时类型，请确保重新运行整个笔记本。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "-B31JBmQm-T6"
            },
            "outputs": [],
            "source": [
                "from cs231n.clip_dino import DINOSegmentation, compute_iou\n",
                "torch.manual_seed(231)\n",
                "np.random.seed(231)\n",
                "dino_segmentation = DINOSegmentation(device, num_classes)\n",
                "dino_segmentation.train(X_train, Y_train, num_iters=500)\n",
                "\n",
                "\n",
                "# Test on first, middle, and last frame\n",
                "ious = []\n",
                "test_fis = [0, train_fi, 98]\n",
                "gt_viz = []\n",
                "pred_viz = []\n",
                "for fi in test_fis:\n",
                "  X_test = davis_ds.process_frames(frames[fi:fi+1], dino_model, device)[0]\n",
                "  Y_test = davis_ds.process_masks(masks[fi:fi+1], device)[0]\n",
                "  Y_pred = dino_segmentation.inference(X_test)\n",
                "  iou = compute_iou(Y_pred, Y_test, num_classes)\n",
                "  ious.append(iou)\n",
                "\n",
                "  gt_viz.append(davis_ds.mask_frame_overlay(Y_test, frames[fi]))\n",
                "  pred_viz.append(davis_ds.mask_frame_overlay(Y_pred, frames[fi]))\n",
                "\n",
                "gt_viz = np.concatenate(gt_viz, 1)\n",
                "pred_viz = np.concatenate(pred_viz, 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "AH5KOhQY4JCb",
                "test": "iou_accuracy"
            },
            "outputs": [],
            "source": [
                "print(f\"Mean IoU on first test frames: {ious[0]:.3f}\")  # should be >0.45\n",
                "print(f\"Mean IoU on last test frames: {ious[2]:.3f}\")  # should be >0.50"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_24lnmJ4FtvA"
            },
            "source": [
                "Now let's visualize the results. Run the two cells below to display the ground truth and predicted segmentation masks for the first, middle, and last frames. Note that the middle frame is part of the training set, while the other frames are unseen.\n",
                "\n",
                "现在让我们将结果可视化。运行下面的两个单元格，以显示第一帧、中间帧和最后一帧的真实分割掩码和预测分割掩码。请注意，中间帧属于训练集，而其他帧是未见过的。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Qai7ItCa7P-D"
            },
            "outputs": [],
            "source": [
                "Image.fromarray(gt_viz)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "YxXEu--C7WYQ"
            },
            "outputs": [],
            "source": [
                "Image.fromarray(pred_viz)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "gmD1T5UoGVHu"
            },
            "source": [
                "Now run the following three cells to evaluate and visualize the entire video. You should achieve a mean IoU greater than 0.45. The saved visualization video may take some time to process in Google Drive, but you can download it to your computer and view it locally.\n",
                "\n",
                "现在运行以下三个单元格来评估和可视化整个视频。你的平均交并比（IoU）应大于0.45。保存的可视化视频在谷歌云端硬盘中可能需要一些时间来处理，但你可以将其下载到电脑上并在本地查看。\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "tIRVNKs3oVPU"
            },
            "outputs": [],
            "source": [
                "# Run on all frames\n",
                "ious = []\n",
                "gt_viz = []\n",
                "pred_viz = []\n",
                "for fi in range(len(frames)):\n",
                "  if fi % 20 == 0:\n",
                "    print(f\"{fi} / {len(frames)}\")\n",
                "  X_test = davis_ds.process_frames(frames[fi:fi+1], dino_model, device)[0]\n",
                "  Y_test = davis_ds.process_masks(masks[fi:fi+1], device)[0]\n",
                "  Y_pred = dino_segmentation.inference(X_test)\n",
                "  iou = compute_iou(Y_pred, Y_test, num_classes)\n",
                "  ious.append(iou)\n",
                "\n",
                "  gt_viz.append(davis_ds.mask_frame_overlay(Y_test, frames[fi]))\n",
                "  pred_viz.append(davis_ds.mask_frame_overlay(Y_pred, frames[fi]))\n",
                "\n",
                "gt_viz = np.stack(gt_viz)  # T x H x W x 3\n",
                "pred_viz = np.stack(pred_viz)  # T x H x W x 3\n",
                "final_viz = np.concatenate([gt_viz, pred_viz], -2)  # T x H x 2W x 3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "bCZkYXtEGkq2",
                "test": "all_frames_iou"
            },
            "outputs": [],
            "source": [
                "print(f\"Mean IoU on all frames: {sum(ious) / len(ious):.3f}\")  # should be >0.55\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "w3DCRaiI8V7J"
            },
            "outputs": [],
            "source": [
                "def write_video_from_array(array, output_path, fps = 12):\n",
                "    T, H, W, _ = array.shape\n",
                "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
                "    out = cv2.VideoWriter(output_path, fourcc, fps, (W, H))\n",
                "    for i in range(T):\n",
                "        frame = array[i]\n",
                "        out.write(frame)\n",
                "    out.release()\n",
                "    print(f\"Video saved to {output_path}\")\n",
                "\n",
                "\n",
                "# It might take a while to process in google drive but you can just download it and watch on your computer\n",
                "write_video_from_array(final_viz, f\"dino_res.mp4\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "s5Z_N_lkHlPW"
            },
            "source": [
                "**Inline Question 5** -\n",
                "\n",
                "If you train a segmentation model on CLIP ViT's patch features, do you expect it to perform better or worse than DINO? Why should that be the case?\n",
                "\n",
                "\n",
                "\n",
                "$\\color{blue}{\\textit Your Answer:}$\n",
                "\n",
                "DINO性能会更好，因为DINO的训练过程要求模型学会从局部特征推断整体，因此和语义分割这样的任务非常契合，而CLIP学习到的更偏向于整体的全局信息，并没有专门训练对局部特征的识别能力。\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "j7joEne5LNYB"
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": [
                {
                    "file_id": "1veMUkShr38Pj6B_7XgEdtioxI6sVfXkU",
                    "timestamp": 1747174681237
                },
                {
                    "file_id": "1EAsfgn8wpiEIIubKR4KuQKxsdCp2rtY2",
                    "timestamp": 1747090108952
                }
            ]
        },
        "kernelspec": {
            "display_name": "cs231n",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
